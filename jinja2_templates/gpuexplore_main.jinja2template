#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <string.h>
#include <cuda.h>
#include <cuda_runtime_api.h>
#include <assert.h>
#include <time.h>
#include <math.h>
#include "{{name}}_gpuexplore.cuh"

#define MIN(a,b) \
   ({ __typeof__ (a) _a = (a); \
       __typeof__ (b) _b = (b); \
     _a < _b ? _a : _b; })

#define MAX(a,b) \
   ({ __typeof__ (a) _a = (a); \
       __typeof__ (b) _b = (b); \
     _a > _b ? _a : _b; })

// GPU constants
__constant__ uint32_t d_kernel_iters;

// test macros
#define PRINTTHREADID()						{printf("Hello thread %d\n", (blockIdx.x*blockDim.x)+threadIdx.x);}
#define PRINTTHREAD(j, i)					{printf("%d: Seen by thread %d: %d\n", (j), (blockIdx.x*blockDim.x)+threadIdx.x, (i));}

/**
 * This macro checks return value of the CUDA runtime call and exits
 * the application if the call failed.
 */
#define CUDA_CHECK_RETURN(value) {											\
	cudaError_t _m_cudaStat = value;										\
	if (_m_cudaStat != cudaSuccess) {										\
		fprintf(stderr, "Error %s at line %d in file %s\n",					\
				cudaGetErrorString(_m_cudaStat), __LINE__, __FILE__);		\
		exit(1);															\
	} }

__inline__ void CUDA_CHECK_FOR_ERROR() {
	cudaError_t err = cudaGetLastError();
	CUDA_CHECK_RETURN(err);
}

int vmem = 0;

// Wrapper around cudaMalloc to count allocated memory and check for error while allocating.
int cudaMallocCount ( void ** ptr,int size) {
	cudaError_t err = cudaSuccess;
	vmem += size;
	err = cudaMalloc(ptr,size);
	if (err) {
		printf("Error %s at line %d in file %s\n", cudaGetErrorString(err), __LINE__, __FILE__);
		exit(1);
	}
	fprintf (stdout, "allocated %d\n", size);
	return size;
}

/**
 * CUDA kernel function to initialise the global memory hash table.
 */
__global__ void init_hash_table(compressed_nodetype *d_q) {
    for (indextype i = GLOBAL_THREAD_ID; i < d_hash_table_size; i += NR_THREADS) {
    	d_q[i] = (compressed_nodetype) {% if compact_hash_table %}EMPTY_COMPRESSED_NODE{% else %}EMPTY_NODE{% endif %};
    }
}

/**
 * CUDA kernel function to count the number of states in the global memory hash table.
 */
__global__ void count_states(compressed_nodetype *d_q, uint64_t *result) {
	if (THREAD_ID == 0) {
		shared[0] = 0;
	}
	__syncthreads();
	shared_inttype localResult = 0;
	for (indextype i = GLOBAL_THREAD_ID; i < {% if compact_hash_table %}d_non_root_offset{% else %}d_hash_table_size{% endif %}; i += NR_THREADS) {
		{% if not compact_hash_table and vectorsize > 63 %}
		if (is_root(d_q[i])) {
		{% else %}
		if (d_q[i] != {% if compact_hash_table %}EMPTY_COMPRESSED_NODE{% else %}EMPTY_NODE{% endif %}) {
		{% endif %}
			localResult++;
		}
	}
	atomicAdd((unsigned int *) &(shared[0]), (unsigned int) localResult);
	__syncthreads();
	if (THREAD_ID == 0) {
		atomicAdd((unsigned long long *) result, (unsigned long long) shared[0]);
	}
}

__global__ void __launch_bounds__(512, 2) gather(compressed_nodetype *d_q, uint8_t *d_contBFS, uint8_t *d_property_violation, volatile uint8_t *d_newstate_flags, shared_inttype *d_worktiles, const uint8_t scan) {
	uint32_t i;
	shared_inttype l;
	{% if vectorsize > 62 %}
	shared_indextype sh_index;
	{% endif %}
	compressed_nodetype tmp;

	// Reset the shared variables preceding the cache, and reset the cache.
	if (THREAD_ID < SH_OFFSET) {
		shared[THREAD_ID] = 0;
	}
	for (i = THREAD_ID; i < (d_shared_cache_size - SH_OFFSET); i += BLOCK_SIZE) {
		shared[SH_OFFSET+i] = EMPTYVECT{% if vectorsize <= 30 or vectorsize > 62 %}32{% else %}64{% endif %};
	}
	__syncthreads();
	if (scan) {
		// Copy the work tile from global memory.
		if (THREAD_ID < OPENTILELEN + LASTSEARCHLEN) {
			shared[OPENTILEOFFSET + THREAD_ID] = d_worktiles[(OPENTILELEN+LASTSEARCHLEN+1) * BLOCK_ID + THREAD_ID];
		}
		if (THREAD_ID == 0) {
			OPENTILECOUNT = d_worktiles[(OPENTILELEN+LASTSEARCHLEN+1) * BLOCK_ID + OPENTILELEN + LASTSEARCHLEN];
		}
	}
	else if (THREAD_ID < OPENTILELEN+LASTSEARCHLEN) {
		// On first run: initialise the work tile to empty.
		shared[OPENTILEOFFSET+THREAD_ID] = 0;
	}
	__syncthreads();
	while (ITERATIONS < d_kernel_iters) {
		if (THREAD_ID == 0 && OPENTILECOUNT < OPENTILELEN && d_newstate_flags[BLOCK_ID] == 1) {
			// Indicate that we are scanning.
			d_newstate_flags[BLOCK_ID] = 2;
			SCAN = 1;
		}
		__syncthreads();
		// Scan the open set for work; we use OPENTILECOUNT to count retrieved elements.
		if (SCAN) {
			indextype last_search_location = (indextype) shared[LASTSEARCHOFFSET + WARP_ID];
			// This block should be able to find a new state.
			bool found_new_state = false;
			for (i = GLOBAL_THREAD_ID; i < {% if not compact_hash_table %}d_hash_table_size{% else %}d_non_root_offset{% endif %} && OPENTILECOUNT < OPENTILELEN; i += NR_THREADS) {
				indextype loc = i + last_search_location;
				if (loc >= {% if not compact_hash_table %}d_hash_table_size{% else %}d_non_root_offset{% endif %}) {
					last_search_location = -i + GLOBAL_THREAD_ID;
					loc = i + last_search_location;
				}
				tmp = d_q[loc];
				l = EMPTYVECT{% if vectorsize <= 30 or vectorsize > 62 %}32{% else %}64{% endif %};
				if ({% if not compact_hash_table %}is_new(tmp){% else %}tmp != EMPTY_COMPRESSED_NODE{% endif %}) {
					found_new_state = true;
					// Try to increment the OPENTILECOUNT counter. If successful, store the state pointer.
					l = atomicAdd((shared_inttype *) &OPENTILECOUNT, 1);
					{% if not compact_hash_table %}
					if (l < OPENTILELEN) {
						d_q[loc] = mark_old(tmp);
					}
					{% endif %}
					shared[OPENTILEOFFSET+l] = tmp;
				}
			}
			if (i < {% if not compact_hash_table %}d_hash_table_size{% else %}d_non_root_offset{% endif %}) {
				last_search_location = i - GLOBAL_WARP_ID;
			}
			else {
				last_search_location = 0;
			}
			if (LANE == 0) {
				shared[LASTSEARCHOFFSET + WARP_ID] = last_search_location;
			}
			if (found_new_state || i < {% if not compact_hash_table %}d_hash_table_size{% else %}d_non_root_offset{% endif %}) {
				WORKSCANRESULT = 1;
			}
		}
		__syncthreads();
		// If work has been retrieved, indicate this.
		if (THREAD_ID == 0) {
			if (OPENTILECOUNT > 0) {
				(*d_contBFS) = 1;
			}
			if (SCAN && WORKSCANRESULT == 0 && d_newstate_flags[BLOCK_ID] == 2) {
				// Scanning has completed and no new states were found by this block.
				// Save this information to prevent unnecessary scanning later on.
				d_newstate_flags[BLOCK_ID] = 0;
			}
			else {
				WORKSCANRESULT = 0;
			}
		}
		{% if vectorsize > 62 %}
		// Fill the cache with the vector trees referred to in the work tile.
		// Create a cooperative group within a warp in which the thread resides.
		thread_block_tile<VECTOR_GROUP_SIZE> gtile = tiled_partition<VECTOR_GROUP_SIZE>(this_thread_block());
		for (i = VECTOR_GROUP_ID; i < OPENTILECOUNT; i += NR_VECTOR_GROUPS_PER_BLOCK) {
			l = FETCH(gtile, d_q, shared[OPENTILEOFFSET+i]);
			if (l == CACHE_FULL) {
				// PLAN B?
			}
			else {
				sh_index = (shared_indextype) l;
			}
			if (gtile.thread_rank() == 0) {
				// Store the address to the tree in the cache in the work tile.
				shared[OPENTILEOFFSET+i] = sh_index;
			}
		}
		__syncthreads();
		{% endif %}
		GENERATE_SUCCESSORS();
		bool performed_work = OPENTILECOUNT != 0;
		__syncthreads();
		// Reset the work tile count
		if (THREAD_ID == 0) {
			OPENTILECOUNT = 0;
		}
		// Start scanning the local cache and write results to the global hash table.
		if (performed_work) {
			FINDORPUT_MANY(d_q, d_newstate_flags);
		}
		__syncthreads();
		// Write 'empty' pointers to unused part of the work tile.
		if (THREAD_ID < OPENTILELEN - OPENTILECOUNT) {
			shared[OPENTILEOFFSET+OPENTILECOUNT+THREAD_ID] = EMPTYVECT{% if vectorsize <= 30 or vectorsize > 62 %}32{% else %}64{% endif %};
		}
		// Ready to start next iteration, if error has not occurred.
		if (THREAD_ID == 0) {
			if (CONTINUE == 2) {
				(*d_contBFS) = 2;
				ITERATIONS = d_kernel_iters;
			}
			else {
				ITERATIONS++;
			}
			CONTINUE = 0;
		}
		__syncthreads();
	}
	// Done. Copy the work tile to global memory.
	if (THREAD_ID < OPENTILELEN+LASTSEARCHLEN) {
		d_worktiles[(OPENTILELEN+LASTSEARCHLEN+1)*BLOCK_ID + THREAD_ID] = shared[OPENTILEOFFSET+THREAD_ID];
	}
	if (THREAD_ID == 0) {
		d_worktiles[(OPENTILELEN+LASTSEARCHLEN+1)*BLOCK_ID + OPENTILELEN + LASTSEARCHLEN] = OPENTILECOUNT;
	}
}

/**
 * Host function that prepares data, copies it to the GPU, and handles the control flow of the model checking.
 */
int main(int argc, char** argv) {
	{% if not compact_hash_table %}
	// Size of global hash table.
	indextype hash_table_size = 0;
	{% else %}
	// Pointer to the start of the internal hash table.
	indextype non_root_offset, d_non_root_offset;
	{% endif %}
	// Number of search iterations in each kernel launch.
	uint32_t kernel_iters = KERNEL_ITERS;
	// Level of verbosity (1=print level progress)
	int verbosity = 0;
	// Clock to measure time.
	clock_t start, stop;
	double runtime = 0.0;

	// Start timer.
	assert((start = clock()) != -1);

	cudaDeviceProp prop;
	int nDevices;

	// Flag to keep track of the progress and whether hash table errors occurred (value == 2).
	uint8_t contBFS, *d_contBFS;
	// Flags to track which blocks have new states.
	uint8_t *d_newstate_flags;
	// Flag to keep track of property verification outcome.
	uint8_t *d_property_violation;
	// Integer to store the amount of states counted in the hash table.
	uint64_t counted_states, *d_counted_states;
	// Space to temporarily store work tiles.
	shared_inttype *d_worktiles;

	// Global hash table.
	compressed_nodetype *d_q;

	const char* help_text =
		"Usage: gpuexplore [OPTIONS]\n"
		"Run state-space exploration on preprocessed SLCO model.\n"
		"options:\n"
		"  -k NUM           Run NUM iterations per kernel launch (default 1).\n"
		"  -q NUM           Allocate NUM integers for the global hash table (default fill the memory).\n"
		"  -v NUM           Change the verbosity level:\n"
		"                      0 - minimal output.\n"
		"                      1 - print sequence number of each kernel launch (search step).\n"
		"                      2 - print number of states in the global hash table after each kernel launch.\n"
		"                      3 - print global hash table content after each kernel launch.\n"
		"  -h, --help, -?   Show this help message.\n";

	int i = 1;
	while (i < argc) {
		if (!strcmp(argv[i],"--help") || !strcmp(argv[i],"-h") || !strcmp(argv[i],"-?")) {
			fprintf(stdout, "%s", help_text);
			exit(0);
		}
		else if (!strcmp(argv[i],"-k")) {
			kernel_iters = atoi(argv[i+1]);
			i += 2;
		}
		else if (!strcmp(argv[i],"-q")) {
			hash_table_size = atoi(argv[i+1]);
			i += 2;
		}
		else if (!strcmp(argv[i],"-v")) {
			verbosity = atoi(argv[i+1]);
			if (verbosity > 3) {
				verbosity = 3;
			}
			i += 2;
		}
		else {
			fprintf(stderr, "ERROR: unrecognised option %s!\n", argv[i]);
			fprintf(stdout, "%s", help_text);
			exit(1);
		}
	}

	// Set continue flag.
	contBFS = 1;
	
	// Query the device properties and determine the data structure sizes.
	cudaGetDeviceCount(&nDevices);
	if (nDevices == 0) {
		fprintf(stderr, "ERROR: No CUDA compatible GPU detected!\n");
		exit(1);
	}
	cudaGetDeviceProperties(&prop, 0);
	fprintf (stdout, "global mem: %lu\n", (uint64_t) prop.totalGlobalMem);
	fprintf (stdout, "shared mem per block: %d\n", (int) prop.sharedMemPerBlock);
	fprintf (stdout, "shared mem per SM: %d\n", (int) prop.sharedMemPerMultiprocessor);
	fprintf (stdout, "max. threads per block: %d\n", (int) prop.maxThreadsPerBlock);
	fprintf (stdout, "max. grid size: %d\n", (int) prop.maxGridSize[0]);
	fprintf (stdout, "nr. of multiprocessors: %d\n", (int) prop.multiProcessorCount);

	// Determine actual number of blocks.
	uint32_t nblocks = MAX(1,MIN(prop.maxGridSize[0], NR_BLOCKS));

	// Allocate memory on the GPU.
	cudaMallocCount((void **) &d_contBFS, sizeof(uint8_t));
	cudaMallocCount((void **) &d_property_violation, sizeof(uint8_t));
	cudaMallocCount((void **) &d_counted_states, sizeof(uint64_t));
	cudaMallocCount((void **) &d_newstate_flags, nblocks * sizeof(uint8_t));
	cudaMallocCount((void **) &d_worktiles, nblocks * (OPENTILELEN+LASTSEARCHLEN+1) * sizeof(shared_inttype));

	// Set data on the GPU to initial values.
	CUDA_CHECK_RETURN(cudaMemset(d_contBFS, 1, sizeof(uint8_t)));
	CUDA_CHECK_RETURN(cudaMemset(d_newstate_flags, 0, nblocks * sizeof(uint8_t)));
	CUDA_CHECK_RETURN(cudaMemset(d_worktiles, 0, nblocks * (OPENTILELEN + LASTSEARCHLEN + 1) * sizeof(shared_inttype)));
	CUDA_CHECK_RETURN(cudaMemset(d_counted_states, 0, sizeof(uint64_t)));

	{% if not compact_hash_table %}
	{% if vectorsize <= 62 %}
	size_t available, total;
	cudaMemGetInfo(&available, &total);
	if (hash_table_size == 0) {
		hash_table_size = total / sizeof(compressed_nodetype);
	}
	size_t el_per_Mb = Mb / sizeof(compressed_nodetype);

	while (cudaMalloc((void **)&d_q, hash_table_size * sizeof(compressed_nodetype)) == cudaErrorMemoryAllocation) {
		hash_table_size -= el_per_Mb;
		if (hash_table_size < el_per_Mb) {
			fprintf(stderr, "ERROR: not enough free memory on the GPU!\n");
			exit(1);
		}
	}
	{% else %}
	// We create a hash table for (non-compact) state vector trees. Its size is limited to 2^31 elements, as a 64-bit element needs to be able to store two pointers, plus two admin bits.
	if (hash_table_size == 0 || hash_table_size > 2147483648) {
		hash_table_size = 2147483648;
	}
	cudaMalloc((void **)&d_q, hash_table_size * sizeof(compressed_nodetype));
	{% endif %}
	{% elif global_memsize == 24 %}
	// We create a global compact hash table for 24 GB. A root table is created that has exactly 2^32 elements, and an internal table is created with 500 million elements.
	cudaMalloc((void **)&d_q, 4831838208 * sizeof(compressed_nodetype));
	non_root_offset = 4294967296;
	{% endif %}

	fprintf (stdout, "Global mem queue size: %lu; Number of entries: %{% if vectorsize <= 30 or compact_hash_table %}l{% endif %}u\n", hash_table_size*sizeof(compressed_nodetype), (indextype) hash_table_size);

	shared_inttype shared_cache_size = (shared_inttype) prop.sharedMemPerMultiprocessor / sizeof(shared_inttype) / 2;
	fprintf (stdout, "Shared mem work tile size: {{tilesize}}\n");
	fprintf (stdout, "Shared mem cache size: %u; Number of entries: %u\n", (uint32_t) (shared_cache_size*sizeof(shared_inttype)), (uint32_t) shared_cache_size{% if vectorsize > 62 %}*3{% endif %});
	fprintf (stdout, "Nr. of blocks: %d; Block size: {{nrthreadsperblock}}; Nr. of kernel iterations: %d\n", nblocks, kernel_iters);

	// Copy symbols.
	cudaMemcpyToSymbol(d_shared_cache_size, &shared_cache_size, sizeof(shared_inttype));
	cudaMemcpyToSymbol(d_kernel_iters, &kernel_iters, sizeof(uint32_t));
	{% if compact_hash_table %}
	cudaMemcpyToSymbol(d_non_root_offset, &non_root_offset, sizeof(indextype));
	{% else %}
	cudaMemcpyToSymbol(d_hash_table_size, &hash_table_size, sizeof(indextype));
	{% endif %}

	// Initialise the hash table.
	init_hash_table<<<NR_BLOCKS, {{nrthreadsperblock}}>>>(d_q);
	CUDA_CHECK_FOR_ERROR();
	store_initial_state<<<1, {{nrthreadsperblock}}, shared_cache_size * sizeof(shared_inttype)>>>(d_q, d_newstate_flags, d_worktiles);

	compressed_nodetype *q_test;
	if (verbosity == 3) {
		q_test = (compressed_nodetype*) malloc(sizeof(compressed_nodetype)*hash_table_size);
	}

	uint32_t iterations_counter = 0;
	uint8_t scan = 1;
	CUDA_CHECK_RETURN(cudaMemset(d_property_violation, 0, sizeof(uint8_t)));
	uint8_t property_violation = 0;

	clock_t exploration_start;
	assert((exploration_start = clock()) != -1);

	while (contBFS == 1) {
		CUDA_CHECK_RETURN(cudaMemset(d_contBFS, 0, sizeof(uint8_t)));
		// TODO: change nr of blocks back to nblocks
		gather<<<1, {{nrthreadsperblock}}, shared_cache_size * sizeof(shared_inttype)>>>(d_q, d_contBFS, d_property_violation, d_newstate_flags, d_worktiles, scan);

		// Copy progress result.
		CUDA_CHECK_RETURN(cudaDeviceSynchronize());
		CUDA_CHECK_RETURN(cudaMemcpy(&contBFS, d_contBFS, sizeof(uint8_t), cudaMemcpyDeviceToHost));
		// if (check_property > 0) {
		// }
		if (verbosity > 0) {
			if (verbosity == 1) {
				fprintf(stdout, "%d\n", iterations_counter++);
			}
			else if (verbosity == 2) {
				cudaMemset(d_counted_states, 0, sizeof(uint64_t));
				count_states<<<((int) prop.multiProcessorCount)*8, 512, 1>>>(d_q, d_counted_states);
				CUDA_CHECK_RETURN(cudaDeviceSynchronize());
				CUDA_CHECK_RETURN(cudaMemcpy(&counted_states, d_counted_states, sizeof(uint64_t), cudaMemcpyDeviceToHost));
				fprintf(stdout, "Nr. of states in hash table: %lu\n", counted_states);
			}
			else if (verbosity == 3) {
				cudaMemcpy(q_test, d_q, hash_table_size * sizeof(compressed_nodetype), cudaMemcpyDeviceToHost);
				print_content_hash_table(stdout, q_test, {% if compact_hash_table %}non_root_offset{% else %}hash_table_size{% endif %});
			}
		}
		scan = 1;
	}

	// Determine runtime.
	stop = clock();
	runtime = (double) (stop-start)/CLOCKS_PER_SEC;
	fprintf(stdout, "Run time: %f\n", runtime);
	runtime = (double) (stop-exploration_start)/CLOCKS_PER_SEC;
	fprintf(stdout, "Exploration time: %f\n", runtime);

	// TODO: Property violation report

	// Report hash table error if required.
	if (contBFS == 2) {
		fprintf(stderr, "ERROR: problem with hash table or caches!\n");
	}

	CUDA_CHECK_RETURN(cudaMemset(d_counted_states, 0, sizeof(indextype)));
	cudaMemset(d_counted_states, 0, sizeof(uint64_t));
	count_states<<<((int) prop.multiProcessorCount)*8, 512, 1>>>(d_q, d_counted_states);
	CUDA_CHECK_RETURN(cudaDeviceSynchronize());
	CUDA_CHECK_RETURN(cudaMemcpy(&counted_states, d_counted_states, sizeof(uint64_t), cudaMemcpyDeviceToHost));
	fprintf(stdout, "Nr. of states in hash table: %lu\n", counted_states);

	return 0;
}
