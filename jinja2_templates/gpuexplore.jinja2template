#include <stdbool.h>
#include <cooperative_groups.h>
using namespace cooperative_groups;

// type of vectortree nodes used.
{% if vectorsize < 32 %}
#define root_nodetype uint32_t
{% else %}
#define nodetype uint64_t
{% if no_compact_hash_table %}
#define root_nodetype uint64_t
{% else %}
#define root_nodetype uint32_t
{% endif %}
{% endif %}
// type of global memory elements used.
{% if no_compact_hash_table %}
{% if vectorsize < 32 %}
#define global_inttype uint32_t
{% else %}
#define global_inttype uint64_t
{% endif %}
{% else %}
#define global_inttype uint32_t
{% endif %}
// type of global memory indices used.
#define indextype uint32_t
{% if not no_compact_hash_table %}
{% if nr_bits_address_internal > 32 %}
#define indextype_internal uint64_t
{% else %}
#define indextype_internal uint32_t
{% endif %}
{% else %}
#define indextype_internal indextype
{% endif %}
// type of shared memory elements used.
{% if vectorsize > 31 and vectorsize < 64 %}
#define shared_inttype uint64_t
{% else %}
#define shared_inttype uint32_t
{% endif %}
// type for shared memory cache indices.
#define shared_indextype uint16_t
// type of state machine state.
{% if max_statesize <= 8 %}
#define statetype uint8_t
{% elif max_statesize <= 16 %}
#define statetype uint16_t
{% elif max_statesize <= 32 %}
#define statetype uint32_t
{% else %}
#define statetype uint64_t
{% endif %}
// types of data elements.
#define elem_inttype int32_t
#define elem_chartype int8_t
#define elem_booltype bool
// type for array and channel buffer indexing.
{% if max_arrayindexsize <= 8 %}
#define array_indextype int8_t
{% elif max_arrayindexsize <= 16 %}
#define array_indextype int16_t
{% else %}
#define array_indextype int32_t
{% endif %}
// type for indexing in variable buffers.
{% if max_buffer_allocs <= 8 %}
#define buffer_indextype int8_t
{% elif max_buffer_allocs <= 16 %}
#define buffer_indextype int16_t
{% else %}
#define buffer_indextype int32_t
{% endif %}
// type for vector nodes IDs.
{% if vectortree|length < 2**8 %}
#define vectornode_indextype uint8_t;
{% elif vectortree|length < 2**16 %}
#define vectornode_indextype uint16_t;
{% elif vectortree|length < 2**32 %}
#define vectornode_indextype uint32_t;
{% elif vectortree|length < 2**64 %}
#define vectornode_indextype uint32_t;
{% endif %}

// GPU constants.
static const int WARP_SIZE = 32;

// GPU configuraton.
static const int KERNEL_ITERS = 1;
static const int NR_OF_BLOCKS = 3120;

// Thread ids and dimensions.
#define GRID_SIZE 				gridDim.x
#define BLOCK_SIZE				blockDim.x

#define BLOCK_ID				blockIdx.x
#define THREAD_ID 				threadIdx.x
#define WARP_ID					(THREAD_ID / WARP_SIZE)
#define GLOBAL_WARP_ID			(((BLOCK_SIZE / WARP_SIZE) * BLOCK_ID) + WARP_ID)
#define NR_WARPS_PER_BLOCK		(BLOCK_SIZE / WARP_SIZE)
#define NR_WARPS				(NR_WARPS_PER_BLOCK * GRID_SIZE)
#define LANE					(THREAD_ID & 0x0000001F)

// Constant representing empty array index entry.
#define EMPTY_INDEX -1
// Constant used to initialise state variables.
#define NO_STATE {{no_state_constant}}
// Empty local cache element (exploits that a vectornode cannot be marked 'new' without being marked 'root')
#define EMPTYVECT32 			0xBFFFFFFF
#define ISVECTORSTATE(t)		((t) != EMPTYVECT32)
#define EMPTYVECT16				0xFFFF
#define CACHE_POINTERS_NEW_LEAF	0x40000000

// Retry constant to determine number of retries for element insertion.
#define RETRYFREQ 7
#define NR_HASH_FUNCTIONS 8
{% if not no_compact_hash_table %}
#define NR_HASH_FUNCTIONS_ROOT 32
{% endif %}
// Number of retries in local cache.
#define CACHERETRYFREQ 20
// Constant to indicate that no more work is required.
#define EXPLORATION_DONE 0x7FFFFFFF

const size_t Mb = 1<<20;

// CONSTANTS FOR SHARED MEMORY CACHES
// Offsets calculations for shared memory arrays
#define HASHCONSTANTSLEN		(2*NR_HASH_FUNCTIONS)
#define OPENTILELEN				{{tilesize}}
#define LASTSEARCHLEN			(BLOCK_SIZE/WARP_SIZE)

// Offsets in shared memory from which loaded data can be read.
#define SH_OFFSET 5
#define HASHCONSTANTSOFFSET 	(SH_OFFSET)
#define OPENTILEOFFSET 			(HASHCONSTANTSOFFSET+HASHCONSTANTSLEN)
#define LASTSEARCHOFFSET		(OPENTILEOFFSET+OPENTILELEN)
#define CACHEOFFSET 			(LASTSEARCHOFFSET+LASTSEARCHLEN)

// Error value to indicate a full global hash table.
#define HASHTABLE_FULL 0

// Shared memory local progress flags
#define ITERATIONS				(shared[0])
#define CONTINUE				(shared[1])
#define OPENTILECOUNT			(shared[2])
#define WORKSCANRESULT			(shared[3])
#define SCAN					(shared[4])

{% if gpuexplore2_succdist %}
// Definitions for open tile element access.
#define	GROUP_TID				(LANE % {{smnames|length}})
#define GROUPS_PER_WARP			(WARP_SIZE / {{smnames|length}})
#define GROUP_ID				(WARP_ID * GROUPS_PER_WARP + LANE / {{smnames|length}})
#define NR_GROUPS				((BLOCK_SIZE / WARP_SIZE) * GROUPS_PER_WARP)

#define THREADINGROUP			(LANE < GROUPS_PER_WARP * {{smnames|length}})

{% endif %}
// CONSTANTS FOR GLOBAL MEMORY HASH TABLE
{% if no_compact_hash_table %}
// Empty hash table element (exploits that a vectornode cannot be marked 'new' without being marked 'root')
{% if vectorsize < 32 %}
#define EMPTYVECTNODE_GLOBAL 	0xBFFFFFFF
{% else %}
#define EMPTYVECTNODE_GLOBAL 	0xBFFFFFFFFFFFFFFF
{% endif %}
{% else %}
// Empty root hash table element
#define EMPTYVECTNODE_GLOBAL	0x0
{% endif %}

// GPU shared memory array.
extern __shared__ volatile shared_inttype shared[];

// Structure of the state vector:
// {{vectorstructure_string}}
{% if not no_smart_fetching %}

// Bitmask to identify parts of the state vector that contain state machine states.
#define VECTOR_SMPARTS			{{smart_vectortree_fetching_bitmask["smstates"]}}
{% endif %}

// *** START FUNCTIONS FOR VECTOR TREE NODES MANIPULATION AND STORAGE TO THE SHARED MEMORY CACHE ***

// Mark state as new or old.
inline __device__ vectornodetype mark_new(root_nodetype node) {
{% if vectorsize < 32 or not no_compact_hash_table %}
	return node | 0x80000000;
{% else %}
	return node | 0x8000000000000000;
{% endif %}
}

inline __device__ vectornodetype mark_old(root_nodetype node) {
{% if vectorsize < 32 or not no_compact_hash_table %}
	return node & 0x7FFFFFFF;
{% else %}
	return node & 0x7FFFFFFFFFFFFFFF;
{% endif %}
}

// Check whether state is new.
inline __device__ bool is_new(root_nodetype node) {
{% if vectorsize < 32 or not no_compact_hash_table %}
	return (node & 0x80000000) != 0;
{% else %}
	return (node & 0x8000000000000000) != 0;
{% endif %}
}

// Check whether the highest 32-bits of a node in shared memory are set as new.
inline __device__ bool head_is_new(shared_inttype node) {
	return (node & 0x80000000) != 0;
}

{% if no_compact_hash_table and vectorsize > 31 %}
// Mark a node as root.
inline __device__ inttype mark_root(root_nodetype node) {
	return node | 0x4000000000000000;
}

// Check whether node is root.
inline __device__ bool is_root(root_nodetype node) {
	return (node & 0x4000000000000000) != 0;
}

{% endif %}
// Check whether the highest 32-bits of a node in shared memory are set as root.
inline __device__ bool head_is_root(shared_inttype node) {
	return (node & 0x40000000) != 0;
}

{% if vectorsize > 62 %}
// Function to traverse one step in state vector tree (stored in shared memory).
inline __device__ shared_indextype sv_step(shared_indextype node_index, bool goright) {
	shared_indextype index = 0;
	shared_inttype tmp = shared[CACHEOFFSET+node_index+2];
	if (!goright) {
		asm("{\n\t"
			" .reg .u32 t1;\n\t"
			" bfe.u32 t1, %1, 15, 15;\n\t"
			" cvt.u16.u32 %0, t1;\n\t"
			"}" : "=h"(index) : "r"(tmp));
	}
	else {
		asm("{\n\t"
			" .reg .u32 t1;\n\t"
			" bfe.u32 t1, %1, 0, 15;\n\t"
			" cvt.u16.u32 %0, t1;\n\t"
			"}" : "=h"(index) : "r"(tmp));
	}
	return index;
}

{% endif %}
{% if vectorsize > 31 %}
// Get left or right half of a vectornode
inline __device__ shared_inttype get_left(nodetype node) {
	shared_inttype result;
	asm("{\n\t"
		" .reg .u64 t1;\n\t"
		" bfe.u64 t1, %1, 32, 32;\n\t"
		" cvt.u32.u64 %0, t1;\n\t"
		"}" : "=r"(result) : "l"(node));
	return result;
}

inline __device__ shared_inttype get_right(nodetype node) {
	shared_inttype result;
	asm("{\n\t"
		" cvt.u32.u64 %0, %1;\n\t"
		"}" : "=r"(result) : "l"(node));
	return result;
}

{% endif %}
// Retrieval functions for vector parts from shared memory, ignoring shared memory node pointers (cache pointers).
inline __device__ nodetype get_vectorpart(shared_indextype node_index, vectornode_indextype part_id) {
	switch (part_id) {
	  {% for i in range(0,vectorstructure|length) %}
	  case {{i}}:
	  	return get_vectorpart_{{i}}(node_index);
	  {% endfor %}
	  default:
	  	return 0;
	}
}
{% for i in range(0,vectorstructure|length) %}

inline __device__ nodetype get_vectorpart_{{i}}(shared_indextype node_index) {
	{% if vectorsize <= 30 %}
	return shared[CACHEOFFSET+node_index];
	{% else %}
	{% if i|get_vector_tree_to_part_navigation|length > 0 %}
	shared_indextype index = node_index;
	{% endif %}
	{% for b in i|get_vector_tree_to_part_navigation %}
	index = sv_step(index, {% if b %}true{% else %}false{% endif %});
	{% endfor %}
	nodetype part;
	asm("{\n\t"
		" mov.b64 %0,{ %1, %2 };\n\t"
		"}" : "=l"(part) : "r"(shared[CACHEOFFSET+{% if i|get_vector_tree_to_part_navigation|length > 0 %}index{% else %}node_index{% endif %}]), "r"(shared[CACHEOFFSET+{% if i|get_vector_tree_to_part_navigation|length > 0 %}index{% else %}node_index{% endif %}+1]));
	return part;
	{% endif %}
}
{% endfor %}

{% if vectorsize > 62 %}
// Retrieval functions for vector tree nodes from shared memory, including shared memory node pointers (cache pointers).
inline __device__ void get_vectortree_node(inttype *node, shared_inttype *d_cachepointers, shared_indextype node_index, vectornode_indextype i) {
	switch (i) {
	  {% for i in range(0,vectortree|length) %}
	  case {{i}}:
	  	get_vectortree_node_{{i}}(node, d_cachepointers, node_index);
	  {% endfor %}
	  default:
	  	return;
	}
}
{% for i in range(0,vectortree|length) %}

inline __device__ void get_vectortree_node_{{i}}(inttype *node, shared_inttype *d_cachepointers, shared_indextype node_index) {
	{% if i|get_vector_tree_to_node_navigation|length > 0 %}
	shared_indextype index = node_index;
	{% endif %}
	{% for b in i|get_vector_tree_to_node_navigation %}
	index = sv_step(index, {% if b %}true{% else %}false{% endif %});
	{% endfor %}
	{% if vectorsize < 32 %}
	(*node) = shared[CACHEOFFSET+{% if i|get_vector_tree_to_node_navigation|length > 0 %}index{% else %}node_index{% endif %}];
	{% else %}
	asm("{\n\t"
		" mov.b64 %0,{ %1, %2 };\n\t"
		"}" : "=l"(*node) : "r"(shared[CACHEOFFSET+{% if i|get_vector_tree_to_node_navigation|length > 0 %}index{% else %}node_index{% endif %}]), "r"(shared[CACHEOFFSET+{% if i|get_vector_tree_to_node_navigation|length > 0 %}index{% else %}node_index{% endif %}+1]));
	{% endif %}
	*d_cachepointers = shared[CACHEOFFSET+{% if i|get_vector_tree_to_node_navigation|length > 0 %}index{% else %}node_index{% endif %}+2];
}
{% endfor %}

// Cache pointers set functions.
inline __device__ void set_left_cache_pointer(shared_inttype *pointers, shared_indextype new_pointer) {
	asm("{\n\t"
		" bfi.u32 %0, %1, %0, 15, 15;\n\t"
		"}" : "+r"(*pointers) : "h"(new_pointer));
}

inline __device__ void set_right_cache_pointer(shared_inttype *pointers, shared_indextype new_pointer) {
	asm("{\n\t"
		" bfi.u32 %0, %1, %0, 0, 15;\n\t"
		"}" : "+r"(*pointers) : "h"(new_pointer));
}

inline __device__ bool cache_pointers_are_marked_new_leaf(shared_inttype pointers) {
	return pointers == CACHE_POINTERS_NEW_LEAF;
}

inline __device__ void set_cache_pointers_to_global_address(shared_inttype *pointers, indextype address) {
	// The highest bit in pointers is set to indicate that it now stores a global memory address.
	*pointers = address | 0x80000000;
}

inline __device__ bool cache_pointers_contain_global_address(shared_inttype pointers) {
	return (pointers & 0x80000000) != 0;
}

// Vectornode reset functions.
inline __device__ void reset_left_in_vectortree_node(nodetype *node) {
	asm("{\n\t"
		" bfi.u64 %0, 0, %0, {{nr_bits_address_internal}}, {{nr_bits_address_internal}};\n\t"
		"}" : "+l"(*node));
}

inline __device__ void reset_right_in_vectortree_node(inttype *node) {
	asm("{\n\t"
		" bfi.u64 %0, 0, %0, 0, {{nr_bits_address_internal}};\n\t"
		"}" : "+l"(*node));
}

// Vectornode set functions.
inline __device__ void set_left_in_vectortree_node(inttype *node, indextype address) {
	asm("{\n\t"
		" bfi.u64 %0, %1, %0, {{nr_bits_address_internal}}, {{nr_bits_address_internal}};\n\t"
		"}" : "+l"(*node) : "r"(address));
}

inline __device__ void set_right_in_vectortree_node(inttype *node, indextype address) {
	asm("{\n\t"
		" bfi.u64 %0, %1, %0, 0, {{nr_bits_address_internal}};\n\t"
		"}" : "+l"(*node) : "r"(address));
}

// Vectornode get functions.
inline __device__ indextype get_pointer_from_vectortree_node(inttype node, bool choice) {
	indextype result;
	asm("{\n\t"
		" .reg .u64 t1;\n\t"
		" bfe.u64 t1, %1, %2, {{nr_bits_address_internal}};\n\t"
		" cvt.u32.u64 %0, t1;\n\t"
		"}" : "+r"(result) : "l"(node), "r"((1-(choice ? 1 : 0))*{{nr_bits_address_internal}}));
	return result;
}

// Vectornode check for a left or right pointer gap.
inline __device__ bool vectortree_node_contains_left_gap(nodetype node) {
	return (node & {{((nr_bits_address_internal|pow2)-1)|bitshift_left(nr_bits_address_internal)|hexa}}) == {{((nr_bits_address_internal|pow2)-1)|bitshift_left(nr_bits_address_internal)|hexa}};
}

inline __device__ bool vectortree_node_contains_right_gaps(nodetype node) {
	return (node & {{((nr_bits_address_internal|pow2)-1)|hexa}}) == {{((nr_bits_address_internal|pow2)-1)|hexa}};
}

{% endif %}
// Store a vectortree node in the cache.
// Return address if successful, HASHTABLE_FULL if cache is full.
{% if vectorsize < 64 %}
inline __device__ shared_indextype STOREINCACHE(inttype node) {
{% else %}
inline __device__ bool STOREINCACHE(inttype node, shared_inttype cache_pointers) {
{% endif %}
	uint8_t i = 0;
	shared_indextype addr;
	shared_inttype element;
	{% if vectorsize > 63 %}
	shared_inttype part1, part2;

	// Split the node in two.
	part1 = get_left(node);
	part2 = get_right(node);
	{% endif %}
	while (i < CACHERETRYFREQ) {
		addr = 3*(CACHE_HASH({% if vectorsize < 64 %}node{% else %}part1{% endif %}) % (d_shared_q_size - CACHEOFFSET) / 3);
		element = atomicCAS((shared_inttype *) &(shared[CACHEOFFSET+addr]), EMPTYVECT32, {% if vectorsize < 64%}node{% else %}part1{% endif %});
		if (element == EMPTYVECT_32 || element == {% if vectorsize < 64%}node{% else %}part1{% endif %}) {
			// Successful storage.
			{% if vectorsize > 63 %}
			element = atomicCAS((shared_inttype *) &(shared[CACHEOFFSET+addr+1]), EMPTYVECT32, part2);
			if (element == EMPTYVECT32 || element == part2) {
				// Successful storage.
				element = atomicCAS((shared_inttype *) &(shared[CACHEOFFSET+addr+2], EMPTYVECT32, cache_pointers);
				if (element == EMPTYVECT32 || element == cache_pointers) {
					// Storage of node successful.
					return addr;
				}
				else {
					// Storage of node after all not successful. Try another hash value.
					i++;
					continue;
				}
			}
			else {
				// Storage of node after all not successful. Try another hash value.
				i++;
				continue;
			}
			{% else %}
			return addr;
			{% endif %}
		}
		else {
			// Storage of node after all not successful. Try another hash value.
			i++;		
		}
	}
	// Storage of node not successful. We conclude that the cache is full.
	return HASHTABLE_FULL;
}

// *** END FUNCTIONS FOR VECTOR TREE NODES MANIPULATION AND STORAGE TO THE SHARED MEMORY CACHE ***

// *** START KERNELS FOR VECTOR TREE NODES STORAGE AND RETRIEVAL TO/FROM THE GLOBAL MEMORY HASH TABLE ***

{% if not no_compact_hash_table %}
// Bit left rotation function.
inline __device__ inttype rot(const inttype x, uint8_t i) {
	return (x << i) | (x >> (58-i));
}

// XOR two times bit left rotation function.
inline __device__ inttype xor_rot2(inttype x, uint8_t a, uint8_t b) {
	inttype y = x ^ rot(x,a);
	y = y ^ rot(y,b);
	return y &= 0x03FFFFFFFFFFFFFFL;
}

// Inverse function for XOR two times bit left rotation, mod 58.
inline __device__ inttype xor_rot2_inv(inttype x, uint8_t a, uint8_t b) {
	inttype y = xor_rot2(x, a, b);
	uint8_t a1 = (8*a) & 0x03FFFFFFFFFFFFFFL;
	uint8_t b1 = (8*b) & 0x03FFFFFFFFFFFFFFL;
	y = xor_rot2(y, a1, b1);
	a1 = (a1+a1) & 0x03FFFFFFFFFFFFFFL;
	b1 = (b1+b1) & 0x03FFFFFFFFFFFFFFL;
	y = xor_rot2(y, a1, b1);
	a1 = (a1+a1) & 0x03FFFFFFFFFFFFFFL;
	b1 = (b1+b1) & 0x03FFFFFFFFFFFFFFL;
	y = xor_rot2(y, a1, b1);
	return y;
}

// Initial bitmixer function.
inline __device__ inttype HASH_INIT(inttype node) {
	inttype node1 = xor_rot2(node, 38, 14);
	node1 ^= 0xD1B54A32D192ED03L;
	node1 *= 0xAEF17502108EF2D9L;
	return node1;
}

// Hash functions.
inline __device__ inttype HASH(uint8_t id, inttype node) {
	inttype node1 = node;
	switch (id) {
		case 0:
			{{12|cuda_xor_lr(3)}}
			{{37|cuda_xor_lr(3)}}
			node1 *= 0x346d5269d6a44c1L;
			node1 &= 0x03ffffffffffffffL;
			{{43|cuda_xor_r3(31, 23, 3)}}
			node1 *= 0x36d3c2b4804a8e1L;
			node1 &= 0x03ffffffffffffffL;
			{{28|cuda_xor_r(3)}}
			break;
		case 1:
			{{10|cuda_xor_lr(3)}}
			{{35|cuda_xor_lr(3)}}
			node1 *= 0x3866c0692e5e421L;
			node1 &= 0x03ffffffffffffffL;
			{{40|cuda_xor_r3(34, 19, 3)}}
			node1 *= 0x39838add4d38e61L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r(3)}}
			break;
		case 2:
			{{14|cuda_xor_lr(3)}}
			{{39|cuda_xor_lr(3)}}
			node1 *= 0x3a5787914312801L;
			node1 &= 0x03ffffffffffffffL;
			{{44|cuda_xor_r3(30, 15, 3)}}
			node1 *= 0x3afb7bb024c4681L;
			node1 &= 0x03ffffffffffffffL;
			{{30|cuda_xor_r(3)}}
			break;
		case 3:
			{{13|cuda_xor_lr(3)}}
			{{34|cuda_xor_lr(3)}}
			node1 *= 0x3b7e13a202e41c1L;
			node1 &= 0x03ffffffffffffffL;
			{{39|cuda_xor_r3(33, 25, 3)}}
			node1 *= 0x3be88e9173fb7c1L;
			node1 &= 0x03ffffffffffffffL;
			{{28|cuda_xor_r(3)}}
			break;
		case 4:
			{{11|cuda_xor_lr(3)}}
			{{35|cuda_xor_lr(3)}}
			node1 *= 0x3c4109af0f01241L;
			node1 &= 0x03ffffffffffffffL;
			{{23|cuda_xor_r3(30, 26, 3)}}
			node1 *= 0x3c8bbb336719fc1L;
			node1 &= 0x03ffffffffffffffL;
			{{20|cuda_xor_r(3)}}
			break;
		case 5:
			{{15|cuda_xor_lr(3)}}
			{{33|cuda_xor_lr(3)}}
			node1 *= 0x3ccba0994f7af81L;
			node1 &= 0x03ffffffffffffffL;
			{{41|cuda_xor_r3(36, 20, 3)}}
			node1 *= 0x3d02e8d5ad09d61L;
			node1 &= 0x03ffffffffffffffL;
			{{23|cuda_xor_r(3)}}
			break;
		case 6:
			{{34|cuda_xor_lr(3)}}
			{{12|cuda_xor_lr(3)}}
			node1 *= 0x3d3335b9a01aac1L;
			node1 &= 0x03ffffffffffffffL;
			{{38|cuda_xor_r3(42, 24, 3)}}
			node1 *= 0x3d5dc5b2dd89301L;
			node1 &= 0x03ffffffffffffffL;
			{{29|cuda_xor_r(3)}}
			break;
		case 7:
			{{23|cuda_xor_lr(3)}}
			{{35|cuda_xor_lr(3)}}
			node1 *= 0x3d839037059b321L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r3(46, 23, 3)}}
			node1 *= 0x3da557a7a356621L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r(3)}}
			break;
		case 8:
			{{31|cuda_xor_lr(3)}}
			{{34|cuda_xor_lr(3)}}
			node1 *= 0x3dc3b70f5d13b41L;
			node1 &= 0x03ffffffffffffffL;
			{{29|cuda_xor_r3(40, 20, 3)}}
			node1 *= 0x3ddf2c9626244a1L;
			node1 &= 0x03ffffffffffffffL;
			{{27|cuda_xor_r(3)}}
			break;
		case 9:
			{{24|cuda_xor_lr(3)}}
			{{20|cuda_xor_lr(3)}}
			node1 *= 0x3df81dfdd3d5e01L;
			node1 &= 0x03ffffffffffffffL;
			{{40|cuda_xor_r3(33, 25, 3)}}
			node1 *= 0x3e0ee0bdbcd6ea1L;
			node1 &= 0x03ffffffffffffffL;
			{{30|cuda_xor_r(3)}}
			break;
		case 10:
			{{10|cuda_xor_lr(3)}}
			{{19|cuda_xor_lr(3)}}
			node1 *= 0x3e23ba68e0e7381L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r3(46, 23, 3)}}
			node1 *= 0x3e36e66407cf201L;
			node1 &= 0x03ffffffffffffffL;
			{{28|cuda_xor_r(3)}}
			break;
		case 11:
			{{14|cuda_xor_lr(3)}}
			{{30|cuda_xor_lr(3)}}
			node1 *= 0x3e48961660141a1L;
			node1 &= 0x03ffffffffffffffL;
			{{43|cuda_xor_r3(12, 26, 3)}}
			node1 *= 0x3e58f4f3df8a861L;
			node1 &= 0x03ffffffffffffffL;
			{{31|cuda_xor_r(3)}}
			break;
		case 12:
			{{19|cuda_xor_lr(3)}}
			{{41|cuda_xor_lr(3)}}
			node1 *= 0x3e68266fecc08a1L;
			node1 &= 0x03ffffffffffffffL;
			{{24|cuda_xor_r3(39, 21, 3)}}
			node1 *= 0x3e764a9bd2ba4c1L;
			node1 &= 0x03ffffffffffffffL;
			{{25|cuda_xor_r(3)}}
			break;
		case 13:
			{{28|cuda_xor_lr(3)}}
			{{15|cuda_xor_lr(3)}}
			node1 *= 0x3e837baf5cdd9e1L;
			node1 &= 0x03ffffffffffffffL;
			{{32|cuda_xor_r3(40, 24, 3)}}
			node1 *= 0x3e8fd1c4d2d9d01L;
			node1 &= 0x03ffffffffffffffL;
			{{27|cuda_xor_r(3)}}
			break;
		case 14:
			{{43|cuda_xor_lr(3)}}
			{{32|cuda_xor_lr(3)}}
			node1 *= 0x3e9b61d7f9527a1L;
			node1 &= 0x03ffffffffffffffL;
			{{31|cuda_xor_r3(46, 28, 3)}}
			node1 *= 0x3ea63db9f9aa901L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r(3)}}
			break;
		case 15:
			{{35|cuda_xor_lr(3)}}
			{{42|cuda_xor_lr(3)}}
			node1 *= 0x3eb074fc94e0161L;
			node1 &= 0x03ffffffffffffffL;
			{{25|cuda_xor_r3(44, 29, 3)}}
			node1 *= 0x3eba165b7e383e1L;
			node1 &= 0x03ffffffffffffffL;
			{{28|cuda_xor_r(3)}}
			break;
	}
	return node1;
}

// Inverse hash functions.
inline __device__ inttype HASH_INVERSE(uint8_t id, inttype node) {
	inttype node1 = node;
	switch (id) {
		case 0:
			{{28|cuda_xor_r_inv(3)}}
			node1 *= 0x3e340cf8be69b21L;
			node1 &= 0x03ffffffffffffffL;
			{{43|cuda_xor_r3_inv(31, 23, 3)}}
			node1 *= 0x16e2e65fde04b41L;
			node1 &= 0x03ffffffffffffffL;
			{{37|cuda_xor_lr_inv(3)}}
			{{12|cuda_xor_lr_inv(3)}}
			break;
		case 1:
			{{26|cuda_xor_r_inv(3)}}
			node1 *= 0x27e8bdf6b3595a1L;
			node1 &= 0x03ffffffffffffffL;
			{{40|cuda_xor_r3_inv(34, 19, 3)}}
			node1 *= 0x3234705f3029fe1L;
			node1 &= 0x03ffffffffffffffL;
			{{35|cuda_xor_lr_inv(3)}}
			{{10|cuda_xor_lr_inv(3)}}
			break;
		case 2:
			{{30|cuda_xor_r_inv(3)}}
			node1 *= 0x38516503a7df981L;
			node1 &= 0x03ffffffffffffffL;
			{{44|cuda_xor_r3_inv(30, 15, 3)}}
			node1 *= 0x3880f7d420ed801L;
			node1 &= 0x03ffffffffffffffL;
			{{39|cuda_xor_lr_inv(3)}}
			{{14|cuda_xor_lr_inv(3)}}
			break;
		case 3:
			{{28|cuda_xor_r_inv(3)}}
			node1 *= 0x3a638f15ba85841L;
			node1 &= 0x03ffffffffffffffL;
			{{39|cuda_xor_r3_inv(33, 25, 3)}}
			node1 *= 0x20fabc04158ce41L;
			node1 &= 0x03ffffffffffffffL;
			{{34|cuda_xor_lr_inv(3)}}
			{{13|cuda_xor_lr_inv(3)}}
			break;
		case 4:
			{{20|cuda_xor_r_inv(3)}}
			node1 *= 0x3592d3c27c27041L;
			node1 &= 0x03ffffffffffffffL;
			{{23|cuda_xor_r3_inv(30, 26, 3)}}
			node1 *= 0x4f6952eaf8fdc1L;
			node1 &= 0x03ffffffffffffffL;
			{{35|cuda_xor_lr_inv(3)}}
			{{11|cuda_xor_lr_inv(3)}}
			break;
		case 5:
			{{23|cuda_xor_r_inv(3)}}
			node1 *= 0x1a0c447ad94c6a1L;
			node1 &= 0x03ffffffffffffffL;
			{{41|cuda_xor_r3_inv(36, 20, 3)}}
			node1 *= 0x2a87df258789081L;
			node1 &= 0x03ffffffffffffffL;
			{{33|cuda_xor_lr_inv(3)}}
			{{15|cuda_xor_lr_inv(3)}}
			break;
		case 6:
			{{29|cuda_xor_r_inv(3)}}
			node1 *= 0xa6775beb906d01L;
			node1 &= 0x03ffffffffffffffL;
			{{38|cuda_xor_r3_inv(42, 24, 3)}}
			node1 *= 0x37e24eee615e541L;
			node1 &= 0x03ffffffffffffffL;
			{{12|cuda_xor_lr_inv(3)}}
			{{34|cuda_xor_lr_inv(3)}}
			break;
		case 7:
			{{26|cuda_xor_r_inv(3)}}
			node1 *= 0x33b48646b8f9de1L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r3_inv(46, 23, 3)}}
			node1 *= 0x14eaae5968790e1L;
			node1 &= 0x03ffffffffffffffL;
			{{35|cuda_xor_lr_inv(3)}}
			{{23|cuda_xor_lr_inv(3)}}
			break;
		case 8:
			{{27|cuda_xor_r_inv(3)}}
			node1 *= 0x1fdf88f19a49f61L;
			node1 &= 0x03ffffffffffffffL;
			{{29|cuda_xor_r3_inv(40, 20, 3)}}
			node1 *= 0xdbbcfbf69154c1L;
			node1 &= 0x03ffffffffffffffL;
			{{34|cuda_xor_lr_inv(3)}}
			{{31|cuda_xor_lr_inv(3)}}
			break;
		case 9:
			{{30|cuda_xor_r_inv(3)}}
			node1 *= 0x2565bb394a9f561L;
			node1 &= 0x03ffffffffffffffL;
			{{40|cuda_xor_r3_inv(33, 25, 3)}}
			node1 *= 0x209a299946a201L;
			node1 &= 0x03ffffffffffffffL;
			{{20|cuda_xor_lr_inv(3)}}
			{{24|cuda_xor_lr_inv(3)}}
			break;
		case 10:
			{{28|cuda_xor_r_inv(3)}}
			node1 *= 0x2749093cc470e01L;
			node1 &= 0x03ffffffffffffffL;
			{{26|cuda_xor_r3_inv(46, 23, 3)}}
			node1 *= 0x397388d192dcc81L;
			node1 &= 0x03ffffffffffffffL;
			{{19|cuda_xor_lr_inv(3)}}
			{{10|cuda_xor_lr_inv(3)}}
			break;
		case 11:
			{{31|cuda_xor_r_inv(3)}}
			node1 *= 0x31b72238ae7fba1L;
			node1 &= 0x03ffffffffffffffL;
			{{43|cuda_xor_r3_inv(12, 26, 3)}}
			node1 *= 0x17db804c1d6e261L;
			node1 &= 0x03ffffffffffffffL;
			{{30|cuda_xor_lr_inv(3)}}
			{{14|cuda_xor_lr_inv(3)}}
			break;
		case 12:
			{{25|cuda_xor_r_inv(3)}}
			node1 *= 0x17cd12a8d2eeb41L;
			node1 &= 0x03ffffffffffffffL;
			{{24|cuda_xor_r3_inv(39, 21, 3)}}
			node1 *= 0xdda8affbefdb61L;
			node1 &= 0x03ffffffffffffffL;
			{{41|cuda_xor_lr_inv(3)}}
			{{19|cuda_xor_lr_inv(3)}}
			break;
		case 13:
			{{27|cuda_xor_r_inv(3)}}
			node1 *= 0x13d6fbb801b6301L;
			node1 &= 0x03ffffffffffffffL;
			{{32|cuda_xor_r3_inv(40, 24, 3)}}
			node1 *= 0x34aade611b82a21L;
			node1 &= 0x03ffffffffffffffL;
			{{15|cuda_xor_lr_inv(3)}}
			{{28|cuda_xor_lr_inv(3)}}
			break;
		case 14:
			{{26|cuda_xor_r_inv(3)}}
			node1 *= 0x303124e6af65701L;
			node1 &= 0x03ffffffffffffffL;
			{{31|cuda_xor_r3_inv(46, 28, 3)}}
			node1 *= 0x2b47151bd0a7c61L;
			node1 &= 0x03ffffffffffffffL;
			{{32|cuda_xor_lr_inv(3)}}
			{{43|cuda_xor_lr_inv(3)}}
			break;
		case 15:
			{{28|cuda_xor_r_inv(3)}}
			node1 *= 0x1941bdcc12c0021L;
			node1 &= 0x03ffffffffffffffL;
			{{25|cuda_xor_r3_inv(44, 29, 3)}}
			node1 *= 0x998456ffaa62a1L;
			node1 &= 0x03ffffffffffffffL;
			{{42|cuda_xor_lr_inv(3)}}
			{{35|cuda_xor_lr_inv(3)}}
			break;
	}
	return node1;
}

// Retrieve ID of used hash function from a compressed vectortree node.
// Non-root version.
inline __device__ uint8_t get_hash_id(global_inttype n) {
	return (uint8_t) (n >> 29);
}

// Root version.
inline __device__ uint8_t get_hash_id_root(global_inttype n) {
	uint8_t hid = (uint8_t) (n >> 27);
	// Remove the 'new' flag.
	return (hid & 0x0F);
}

// Reconstruct uncompressed vectortree node from the given compressed vectortree node and the address at which it is stored in the global memory hash table.
// Non-root version.
inline __device__ inttype get_uncompressed_node(global_inttype n, indextype i) {
	inttype e = (((inttype) i) << 29);
	// Obtain hash function ID.
	uint8_t hid = (uint8_t) (n >> 29);
	// Remove three bits for hash function ID.
	e |= (inttype) (n & 0x1FFFFFFF);
	// Retrieve uncompressed node and return it.
	return HASH_INVERSE(hid, e);
}

// Root version.
inline __device__ inttype get_uncompressed_node_root(global_inttype n, indextype i) {
	inttype e = (((inttype) i) << 27);
	// Obtain hash function ID.
	uint8_t hid = (uint8_t) (n >> 27);
	hid &= 0x0F;
	// Remove four bits for hash function ID + one for 'new' flag.
	e |= (inttype) (n & 0x07FFFFFF);
	// Retrieve uncompressed node and return it.
	return HASH_INVERSE(hid, e);
}

// Extract a global memory hash table index from a given hash.
// Non-root version.
inline __device__ indextype get_index(inttype h) {
	return (indextype) (h >> 29);
}

// Extract a global memory hash table index from a given hash.
// Root version.
inline __device__ indextype get_index_root(inttype h) {
	return (indextype) (h >> 27);
}

// Construct from a hash and a hash function ID a compressed vectortree node.
// Non-root version.
inline __device__ global_inttype get_compressed_node(inttype h, uint8_t hid) {
	global_inttype e = (global_inttype) hid;
	e = (e << 29);
	e |= (global_inttype) (h & 0x000000001FFFFFFF);
	return e;
}

// Root version.
inline __device__ global_inttype get_compressed_node_root(inttype h, uint8_t hid) {
	global_inttype e = (global_inttype) hid;
	e = (e << 27);
	// Mark the node as 'new'.
	e |= 0x80000000;
	e |= (global_inttype) (h & 0x0000000007FFFFFF);
	return e;
}

{% endif %}
// Construct a compact vectortree node, given the hash of a vectortree node and the ID of the used hash function.
// Non-root version.
inline __device__ global_inttype HT_COMPRESS_NODE(inttype node_hash, )

// Retrieve vectortree node at index i of the global memory hash table.
inline __device __ inttype HT_RETRIEVE(inttype *d_q, indextype i{% if not no_compact_hash_table %}, indextype offset{% endif %}) {
	{% if no_compact_hash_table %}
	return d_q[i];
	{% else %}
	uint8_t hid;
	global_inttype j;
	inttype e;
	j = d_q[offset+i];
	if (offset != 0) {
		// We are retrieving a non-root node from the internal nodes table.
		// Extract the hash function ID.
		hid = get_hash_id(j);
		// Construct the complete element.
		e = get_uncompressed_node(j, i);
	}
	else {
		// We are retrieving a root node from the root table.
		// Extract the hash function ID.
		hid = get_hash_id_root(j);
		// Construct the complete element.
		e = get_uncompressed_node_root(j, i);
	}
	// Inverse the element.
	return HASH_INVERSE(e, hid);
	{% endif %}
}

// Find or put a given vectortree node in the global hash table.
inline ___device_ indextype FINDORPUT_SINGLE(inttype *d_q, inttype node, volatile bool *d_newstate_flags{% if not no_compact_hash_table %}, indextype offset{% endif %}, bool claim_work) {
	{% if not no_compact_hash_table %}
	inttype e1, e2;
	global_inttype compressed_node;
	{% endif %}
	indextype addr;
	inttype element;
	shared_inttype shared_addr;
	{% if not no_compact_hash_table %}
	e1 = HASH_INIT(node);
	for (int i = 0; i < (offset == 0 ? NR_HASH_FUNCTIONS_ROOT : NR_HASH_FUNCTIONS); i++) {
	{% else %}
	for (int i = 0; i < NR_HASH_FUNCTIONS, i++) {
	{% endif %}
		{% if not no_compact_hash_table %}
		e2 = HASH(i, e1);
		if (offset == 0) {
			addr = get_index_root(e2);
			compressed_node = get_compressed_node_root(e2, i);
		}
		else {
			addr = get_index(e2);
			compressed_node = get_compressed_node(e2, i);
			// Special case: ensure that no element consisting entirely of zeros is ever written to the non root table (0 represents an empty table slot).
			// If this is about to happen, continue to the next hash function.
			if (i == 0) {
				if ((compressed_node & 0x00000001FFFFFFF) == 0) {
					continue;
				}
			}
		}
		{% else %}
		addr = HASH(i, node);
		{% endif %}
		element = d_q[addr];
		if (element == EMPTYVECTNODE_GLOBAL) {
			element = atomicCAS(&(d_q[addr]), EMPTYVECTNODE_GLOBAL, {% if not no_compact_hash_table %}compressed_node{% else %}node{% endif %});
			if (element == EMPTYVECTNODE_GLOBAL) {
				// Successfully stored the node.
				{% if not no_compact_hash_table %}
				if (offset == 0) {
				{% else %}
				if (is_root(node)) {
				{% endif %}
					// Try to claim the vector for future work. For this, try to increment the OPENTILECOUNT counter.
					if (claim_work && (shared_addr = atomicInc((inttype *) &OPENTILECOUNT)) < OPENTILELEN) {
						// Store pointer to root in the work tile.
						shared[OPENTILEOFFSET+shared_addr] = (shared_inttype) addr;
					}
					else {
						// There is work available for some block.
						d_newstate_flags[(addr / BLOCK_SIZE) % GRID_SIZE] = true;
					}
				}
				return addr;
			}
		}
		if (element == {% if not no_compact_hash_table %}compressed_node{% else %}node{% endif %}) {
			// The node is already stored.
			return addr;
		}
	}
	// Error: hash table considered full.
	return HASHTABLE_FULL;
}

// Find or put all new vectortree nodes stored in the shared memory cache into the global memory hash table.
__device__ bool FINDORPUT_MANY(inttype *d_q, volatile bool *d_newstate_flags, indextype non_root_offset) {
	inttype node;
	indextype addr;
	{% if vectorsize > 63 %}
	shared_inttype node_pointers;
	shared_inttype node_pointers_child;
	bool work_to_do = false;

	if (THREAD_ID == 0) {
		CONTINUE = 0;
	}
	__syncthreads();
	for (shared_indextype i = THREAD_ID{% if vectorsize > 63 %}*3{% endif %}; i < d_shared_q_size; i += BLOCK_SIZE{% if vectorsize > 63 %}*3{% endif %}) {
		node_pointers = shared[CACHEOFFSET+i+2];
		// Check if node is ready for storage. Only new leafs are ready at this point. We rely on old and new non-leafs having pointers with the highest
		// two bits set to zero, empty entries and old leafs having pointers set to zero, and new leafs having pointers set to 0x40000000.
		if (cache_pointers_are_marked_new_leaf(node)) {
			asm("{\n\t"
			" mov.b64 %0,{ %1, %2 };\n\t"
			"}" : "=l"(node) : "r"(shared[CACHEOFFSET+i+1]), "r"(shared[CACHEOFFSET+i]));
			// Store node in hash table.
			addr = FINDORPUT_SINGLE(d_q, node, d_newstate_flags{% if not no_compact_hash_table %}, d_non_root_offset{% endif %}, true);
			// Store global memory address in cache.
			set_cache_pointers_to_global_address(&shared[CACHEOFFSET+i], addr);
		}
		// Node is not ready yet. Check if it can be updated.
		else if (node_pointers != 0) {
			if (head_is_new(shared[CACHEOFFSET+i])) {
				asm("{\n\t"
				" mov.b64 %0,{ %1, %2 };\n\t"
				"}" : "=l"(node) : "r"(shared[CACHEOFFSET+i+1]), "r"(shared[CACHEOFFSET+i]));
				if (vectortree_node_contains_left_gap(node)) {
					// Look up left child and check for presence of global memory address.
					node_pointers_child = sv_step(i, false);
					node_pointers_child = shared[CACHEOFFSET+node_pointers_child+2];
					if (cache_pointers_contain_global_address(node_pointers_child)) {
						set_left_in_vectortree_node(&node, node_pointers_child);
						// Copy back to shared memory.
						shared[CACHEOFFSET+i] = get_left(node);
						shared[CACHEOFFSET+i+1] = get_right[node];
					}
				}
				if (vectortree_node_contains_right_gap(node)) {
					// Look up right child and check for presence of global memory address.
					node_pointers_child = sv_step(i, true);
					node_pointers_child = shared[CACHEOFFSET+node_pointers_child+2];
					if (cache_pointers_contain_global_address(node_pointers_child)) {
						set_right_in_vectortree_node(&node, node_pointers_child);
						// Copy back to shared memory.
						shared[CACHEOFFSET+i] = get_left(node);
						shared[CACHEOFFSET+i+1] = get_right[node];
					}
				}
				// Ready now?
				if (!vectortree_node_contains_left_gap(node) && !vectortree_node_contains_right_gap(node)) {
					// Possibly reset new bit.
					if (!is_root(node)) {
						node = mark_old(node);
					}
					// Store node in hash table.
					addr = FINDORPUT_SINGLE(d_q, node, d_newstate_flags{% if not no_compact_hash_table %}, (is_root(node) ? 0 : d_non_root_offset){% endif %}, true);
					// Store global memory address in cache.
					set_cache_pointers_to_global_address(&(shared[CACHEOFFSET+i]), addr);
				}
				else {
					work_to_do = true;
					CONTINUE = 1;
				}
			}
		}
	}
	__syncthreads();
	while (CONTINUE == 1) {
		if (THREAD_ID == 0) {
			CONTINUE = 0;
		}
		__syncthreads();
		if (work_to_do) {
			work_to_do = false;
			for (shared_indextype i = THREAD_ID*3; i < d_shared_q_size; i += BLOCK_SIZE*3) {
				node_pointers_child = shared[CACHEOFFSET+i];
				// If the node is marked new, this is a node still to be processed.
				if (head_is_new(node_pointers_child)) {
					node_pointers = shared[CACHEOFFSET+i+2];
					// If this is a root node, processing is only required if the pointers do not contain a global address.
					if (!head_is_root(node_pointers_child) || !cache_pointers_contain_global_address(node_pointers)) {
						asm("{\n\t"
						" mov.b64 %0,{ %1, %2 };\n\t"
						"}" : "=l"(node) : "r"(shared[CACHEOFFSET+i+1]), "r"(shared[CACHEOFFSET+i]));
						if (vectortree_node_contains_left_gap(node)) {
							// Look up left child and check for presence of global memory address.
							node_pointers_child = sv_step(i, false);
							node_pointers_child = shared[CACHEOFFSET+node_pointers_child+2];
							if (cache_pointers_contain_global_address(node_pointers_child)) {
								set_left_in_vectortree_node(&node, node_pointers_child);
								// Copy back to shared memory.
								shared[CACHEOFFSET+i] = get_left(node);
								shared[CACHEOFFSET+i+1] = get_right[node];
							}
						}
						if (vectortree_node_contains_right_gap(node)) {
							// Look up right child and check for presence of global memory address.
							node_pointers_child = sv_step(i, true);
							node_pointers_child = shared[CACHEOFFSET+node_pointers_child+2];
							if (cache_pointers_contain_global_address(node_pointers_child)) {
								set_right_in_vectortree_node(&node, node_pointers_child);
								// Copy back to shared memory.
								shared[CACHEOFFSET+i] = get_left(node);
								shared[CACHEOFFSET+i+1] = get_right[node];
							}
						}
						// Ready now?
						if (!vectortree_node_contains_left_gap(node) && !vectortree_node_contains_right_gap(node)) {
							// Possibly reset new bit.
							if (!is_root(node)) {
								node = mark_old(node);
							}
							// Store node in hash table.
							addr = FINDORPUT_SINGLE(d_q, node, d_newstate_flags{% if not no_compact_hash_table %}, (is_root(node) ? 0 : d_non_root_offset){% endif %}, true);
							// Store global memory address in cache.
							set_cache_pointers_to_global_address(&(shared[CACHEOFFSET+i]), addr);
						}
						else {
							work_to_do = true;
							CONTINUE = 1;
						}
					}
				}
			}
		}
		__syncthreads();
	}
	{% else %}
	for (shared_indextype i = THREAD_ID; i < d_shared_q_size; i += BLOCK_SIZE) {
		node = shared[CACHEOFFSET+i];
		if is_new(node) {
			addr = FINDORPUT_SINGLE(d_q, node, d_newstate_flags{% if not no_compact_hash_table %}, 0{% endif %}, true);
		}
	}
	{% endif %}
}

// Auxiliary functions for the fetching of vectortrees from the global hash table. They encode the distribution of vectortree nodes over the threads
// in a vectortree group, and their structural relations with each other.
inline __device__ uint8_t get_vectortree_leaf_parent_thread(uint8_t tid) {
	switch (tid) {
	{% for n in vectortree.keys() if vectortree[n] == [] %}
		case {{vectortree_leaf_thread[n]}}:
			return {{vectortree_T[n]}};
			break;
	{% endfor %}
		default:
			return {{vectortree.keys()|length}};
			break;
	}
}

inline __device__ uint8_t get_vectortree_nonleaf_parent_thread(uint8_t tid) {
	switch (tid) {
	{% for n in vectortree.keys() if vectortree[n] != [] and n > 0 %}
		case {{n}}:
			return {{vectortree_T[n]}};
			break;
	{% endfor %}
		default:
			return {{vectortree.keys()|length}};
			break;
	}
}

inline __device__ uint8_t get_vectortree_nonleaf_left_child_thread(uint8_t tid) {
	switch (tid) {
	{% for n in vectortree.keys() if vectortree[n] != [] %}
		case {{n}}:
			return {% if vectortree[vectortree[n][0]] != [] %}{{vectortree[n][0]}}{% else %}{{vectortree_leaf_thread[vectortree[n][0]]}}{% endif %};
			break;
	{% endfor %}
		default:
			return {{vectortree.keys()|length}};
			break;
	}
}

inline __device__ uint8_t get_vectortree_nonleaf_right_child_thread(uint8_t tid) {
	switch (tid) {
	{% for n in vectortree.keys() if vectortree[n]|length == 2 %}
		case {{n}}:
			return {% if vectortree[vectortree[n][1]] != [] %}{{vectortree[n][1]}}{% else %}{{vectortree_leaf_thread[vectortree[n][1]]}}{% endif %};
			break;
	{% endfor %}
		default:
			return {{vectortree.keys()|length}};
			break;
	}
}
{% if not no_smart_fetching %}

// Auxiliary functions to obtain bitmasks for smart vectortree fetching based on the states of the state machines.
// Given a vector tree node id, return a bitmask expressing which vectorparts are reachable from the node.
inline __device__ uint32_t get_part_reachability(vectornode_indextype nid) {
	switch (nid) {
		{% for i in range(0,vectortree|length) %}
		case {{i}}:
			return {{smart_vectortree_fetching_bitmask[i]}};
		{% endfor %}
		default:
			return 0;
	}
}

// Functions to obtain a bitmask for a given state machine state that indicates which vectorparts are of interest to process outgoing transitions
// of that state.
{% for i in range(0, smnames|length) %}
inline __device__ uint32_t get_part_bitmask_{{state_order[i]|replace("'","_")}}(statetype sid) {
	switch (sid) {
		{% for s in smname_to_object[state_order[i]][1].states %}
		{% set o = smname_to_object[state_order[i]][0] %}
		{% if s|nr_of_transitions_to_be_processed_by(i,o) > 0 %}
		case {{state_id[s]}}:
			return {{s|get_smart_fetching_vectorparts_bitmask(o)}};
		{% endif %}
		{% endfor %}
		default:
			return 0;
	}
}
{% endfor %}

// Function to construct a bitmask for smart fetching, based on the given vectorparts.
inline __device__ uint32_t get_part_bitmask_for_states_in_vectorpart(uint8_t pid, inttype part1, inttype part2) {
	uint32_t result = 0x0;
	statetype s;
	switch (pid) {
		{% for i in range(0, vectorstructure|length) %}
		case {{i}}:
			{% for j in range(0, state_order|length) %}
			{% set PIDs = vectorelem_in_structure_map[state_order[j]] %}
			{% if PIDs[1][0] == i %}
			result = result | get_part_bitmask_{{state_order[j]|replace("'","_")}}(get_{{state_order[j]|replace("'","_")}}(&s, part1, part2));
			{% endif %}
			{% endfor %}
			return result;
		{% endfor %}
		default:
			return result;
	}
}
{% endif %}

// Retrieve a vectortree from the global hash table and store it in the cache. This is performed in a warp-centric way.
// Address addr points to the root of the requested vectortree. The function returns the address of the root of the vectortree in the cache.
inline __device__ shared_indextype FETCH(thread_group treegroup, inttype *d_q, indextype addr{% if not no_compact_hash_table %}, d_non_root_index{% endif %}) {
	inttype node = 0;
	{% if vectorsize > 63 or (vectorsize > 58 and not no_compact_hash_table) %}
	inttype leaf_node = 0;
	inttype node_tmp_1 = 0;
	inttype node_tmp_2 = 0;
	indextype node_addr = 0;
	shared_inttype cache_pointers = 0;
	shared_indextype cache_addr = 0;
	shared_indextype cache_addr_child = 0;
	uint8_t gid = treegroup.thread_rank();
	uint8_t target_thread_id;
	{% if not no_smart_fetching %}
	uint32_t smart_fetching_bitmask = 0x0;
	{% endif %}
	
	if (gid == 0) {
		node = HT_RETRIEVE(d_q, addr{% if not no_compact_hash_table %}, 0{% endif %});
		node_tmp_1 = node;
	}
	{% set bound = 1 %}{% if not no_smart_fetching %}{% set bound = bound + 1 %}{% endif %}
	{% for j in range(0, bound) %}
	{% for i in range(1, vectortree_depth) %}
	{% set lsize = vectortree_level_ids[i]|length %}
	{% set lfirst = vectortree_level_ids[i][0] %}
	{% set llast = vectortree_level_ids[i][vectortree_level_ids[i]|length - 1] %}
	// Obtain node from vectortree parent.
	target_thread_id = {{vectortree.keys()|length}};
	{% if vectortree_level_nr_of_leaves[i] == lsize %}
	if (gid >= {{vectortree_leaf_thread[lfirst]}} && gid <= {{vectortree_leaf_thread[llast]}}) {
		target_thread_id = get_vectortree_leaf_parent_thread(gid);
	}
	{% elif vectortree_level_nr_of_leaves[i] > 0 %}
	if ({% for n in vectortree_level_ids[i] if vectortree[n]|length == 0 %}gid == {{vectortree_leaf_thread[n]}}{% if not loop.last %} || {% endif %}{% endfor %}) {
		target_thread_id = get_vectortree_leaf_parent_thread(gid);
	}
	{% endif %}
	{% if vectortree_level_nr_of_leaves[i] < lsize %}
	{% if vectortree_level_nr_of_leaves[i] > 0 %}	else {% else %}	{% endif %}if (gid >= {{lfirst}} && gid <= {{llast}}) {
		target_thread_id = gid;
	}
	{% endif %}
	treegroup.sync();
	node_tmp_2 = treegroup.shfl(node_tmp_1, target_thread_id);
	// Process the received node, if applicable.
	if (target_thread_id != {{vectortree.keys()|length}}) {
		node_addr = get_pointer_from_vectortree_node(node_tmp_2, false{% for n in vectortree_level_ids[i] if (vectortree[vectortree_T[n]]|length == 2 and vectortree[vectortree_T[n]][vectortree[vectortree_T[n]]|length-1] == n) %} || gid == {% if vectortree[n] == [] %}{{get_vectortree_leaf_parent_thread[n]}}{% else %}{{n}}{% endif %}{% endfor %});	
	{% if vectortree_level_nr_of_leaves[i] == lsize %}
		{% if not no_smart_fetching %}
		if (({% if j == 0 %}{{smart_vectortree_fetching_bitmask[0]}}{% else %}smart_fetching_bitmask{% endif %} & (1 << (31-gid))) != 0x0) {
			leaf_node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
			// Store the leaf node in the cache.
			cache_pointers = 0;
			cache_addr = STOREINCACHE(leaf_node, cache_pointers);
		}
		{% else %}
		leaf_node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
		// Store the leaf node in the cache.
		cache_pointers = 0;
		cache_addr = STOREINCACHE(leaf_node, cache_pointers);
		{% endif %}
	{% elif vectortree_level_nr_of_leaves[i] > 0 %}
		if ({% for n in vectortree_level_ids[i] if vectortree[n]|length == 0 %}gid == {{vectortree_leaf_thread[n]}}{% if not loop.last %} || {% endif %}{% endfor %}) {
			{% if not no_smart_fetching %}
			if (({% if j == 0 %}{{smart_vectortree_fetching_bitmask[0]}}{% else %}smart_fetching_bitmask{% endif %} & (1 << (31-gid))) != 0x0) {
				leaf_node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
				// Store the leaf node in the cache.
				cache_pointers = 0;
				cache_addr = STOREINCACHE(leaf_node, cache_pointers);
			}
			{% else %}
			leaf_node = d_q[node_addr];
			// Store the leaf node in the cache.
			cache_pointers = 0;
			cache_addr = STOREINCACHE(leaf_node, cache_pointers);
			{% endif %}
		}
		else {
			{% if not no_smart_fetching and j == 1 %}
			if (node == 0 && ({% if j == 0 %}{{smart_vectortree_fetching_bitmask[0]}}{% else %}smart_fetching_bitmask{% endif %} & get_part_reachability(gid)) != 0x0) {
				node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
				node_tmp_1 = node;
			}
			{% else %}
			node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
			node_tmp_1 = node;
			{% endif %}
		}
	{% else %}
		{% if not no_smart_fetching and j == 1 %}
		if (node == 0 && ({% if j == 0 %}{{smart_vectortree_fetching_bitmask[0]}}{% else %}smart_fetching_bitmask{% endif %} & get_part_reachability(gid)) != 0x0) {
			node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
			node_tmp_1 = node;
		}
		{% else %}
		node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, d_non_root_offset){% endif %});
		node_tmp_1 = node;
		{% endif %}
	{% endif %}
	}
	{% endfor %}
	{% if not no_smart_fetching and j == 0 %}
	// Fetch the vectorpart to the right.
	node_tmp_1 = leaf_node;
	treegroup.sync();
	if (cache_addr != 0) {
		node_tmp_2 = treegroup.shfl(node_tmp_1, gid+1);
	}
	// Construct smart fetching bitmask.
	smart_fetching_bitmask = get_part_bitmask_for_states_in_vectorpart(gid, leaf_node, node_tmp_2);
	// Now merge all results of the different threads, resulting in the final bitmask.
	treegroup.sync();
	for (target_thread_id = treegroup.size()/2; target_thread_id > 0; target_thread_id /= 2) {
		smart_fetching_bitmask = smart_fetching_bitmask | treegroup.shfl_xor(smart_fetching_bitmask, target_thread_id);
	}
	// Finally discard the previously retrieved vectorpart ids from the bitmask.
	smart_fetching_bitmask = smart_fetching_bitmask & ~({{smart_vectortree_fetching_bitmask[0]}});
	// Set node_tmp_1 back to retrieved node.
	if (node != 0) {
		node_tmp_1 = node;
	}
	{% elif not no_smart_fetching and j == 1 %}
	// Add the initially retrieved vectorpart ids to the bitmask.
	smart_fetching_bitmask = smart_fetching_bitmask | {{smart_vectortree_fetching_bitmask[0]}};
	{% endif %}
	{% endfor %}
	cache_pointers = 0;
	{% for i in range(0, vectortree_depth-1)|reverse %}
	{% set lsize = vectortree_level_ids[i]|length %}
	{% set lfirst = vectortree_level_ids[i][0] %}
	{% set llast = vectortree_level_ids[i][vectortree_level_ids[i]|length - 1] %}
	// Obtain cache address for left child.
	target_thread_id = {{vectortree.keys()|length}};
	{% if vectortree_level_nr_of_leaves[i] == 0 %}
	if (gid >= {{lfirst}} && gid <= {{llast}}) {
		{% if not no_smart_fetching %}
		if ((smart_fetching_bitmask & get_part_reachability(gid)) != 0x0) {
			target_thread_id = get_vectortree_nonleaf_left_child_thread(gid);
		}
		{% else %}
		target_thread_id = get_vectortree_nonleaf_left_child_thread(gid);
		{% endif %}
	}
	{% elif vectortree_level_nr_of_leaves[i] < lsize %}
	if ({% for n in vectortree_level_ids[i] if vectortree[n]|length > 0 %}gid == {{n}}{% if not loop.last %} || {% endif %}{% endfor %}) {
		{% if not no_smart_fetching %}
		if ((smart_fetching_bitmask & get_part_reachability(gid)) != 0x0) {
			target_thread_id = get_vectortree_nonleaf_left_child_thread(gid);
		}
		{% else %}
		target_thread_id = get_vectortree_nonleaf_left_child_thread(gid);
		{% endif %}
	}
	{% endif %}
	treegroup.sync();
	cache_addr_child = treegroup.shfl(cache_addr, target_thread_id);
	// Set the received cache pointer.
	if (target_thread_id != {{vectortree.keys()|length}}) {
		set_left_cache_pointer(&cache_pointers, cache_addr_child);
	}
	{% if vectortree_level_nr_of_nodes_with_two_children[i] > 0 %}
	// Obtain cache address for right child.
	target_thread_id = {{vectortree.keys()|length}};
	{% if vectortree_level_nr_of_nodes_with_two_children[i] == lsize %}
	if (gid >= {{lfirst}} && gid <= {{llast}}) {
		{% if not no_smart_fetching %}
		if ((smart_fetching_bitmask & get_part_reachability(gid)) != 0x0) {
			target_thread_id = get_vectortree_nonleaf_right_child_thread(gid);
		}
		{% else %}
		target_thread_id = get_vectortree_nonleaf_right_child_thread(gid);
		{% endif %}
	}
	{% else %}
	if ({% for n in vectortree_level_ids[i] if vectortree[n]|length == 2 %}gid == {{n}}{% if not loop.last %} || {% endif %}{% endfor %}) {
		{% if not no_smart_fetching %}
		if ((smart_fetching_bitmask & get_part_reachability(gid)) != 0x0) {
			target_thread_id = get_vectortree_nonleaf_left_child_thread(gid);
		}
		{% else %}
		target_thread_id = get_vectortree_nonleaf_left_child_thread(gid);
		{% endif %}
	}
	{% endif %}
	treegroup.sync();
	cache_addr_child = treegroup.shfl(cache_addr, target_thread_id);
	// Set the received cache pointer.
	if (target_thread_id != {{vectortree.keys()|length}}) {
		set_right_cache_pointer(&cache_pointers, cache_addr_child);
	}
	{% endif %}
	// Store the non-leaf node in the cache.
	{% if vectortree_level_nr_of_leaves[i] == 0 %}
	if (gid >= {{lfirst}} && gid <= {{llast}}) {
	{% else %}
	if ({% for n in vectortree_level_ids[i] if vectortree[n]|length > 0 %}gid == {{n}}{% if not loop.last %} || {% endif %}{% endfor %}) {
	{% endif %}
		{% if not no_smart_fetching %}
		if ((smart_fetching_bitmask & get_part_reachability(gid)) != 0x0) {
			node = mark_old(node);
			cache_addr = STOREINCACHE(node, cache_pointers);
		}
		{% else %}
		node = mark_old(node);
		cache_addr = STOREINCACHE(node, cache_pointers);
		{% endif %}
	}
	treegroup.sync();
	{% endfor %}
	{% else %}
	node = HT_RETRIEVE(d_q, node_addr{% if not no_compact_hash_table %}, 0){% endif %});
	cache_addr = STOREINCACHE(node, cache_pointers);
	{% endif %}
	// Obtain cache address of the root and return it.
	cache_addr_child = treegroup.shfl(cache_addr, 0);
	return cache_addr_child;
}

// *** END KERNELS FOR VECTOR TREE NODES STORAGE AND RETRIEVAL ***

// *** START FUNCTIONS FOR INTRA-WARP BITONIC MERGESORT (Fast Segmented Sort on GPUs, Hou et al., 2017) ***

inline __device__ void CMP_SWP(statetype *s0, statetype *s1, shared_indextype *p0, shared_indextype *p1) {
	statetype s_tmp;
	shared_indextype p_tmp;

	if (*s0 > *s1) {
		s_tmp = *s0;
		*s0 = *s1;
		*s1 = s_tmp;
		p_tmp = *p0;
		*p0 = *p1;
		*p1 = p_tmp;
	}
}

inline __device__ void EQL_SWP(statetype *s0, statetype *s1, shared_indextype *p0, shared_indextype *p1) {
	statetype s_tmp;
	shared_indextype p_tmp;

	if (*s0 != *s1) {
		s_tmp = *s0;
		*s0 = *s1;
		*s1 = s_tmp;
		p_tmp = *p0;
		*p0 = *p1;
		*p1 = p_tmp;
	}
}

inline __device__ void SWP(statetype *s0, statetype *s1, shared_indextype *p0, shared_indextype *p1) {
	statetype s_tmp;
	shared_indextype p_tmp;

	s_tmp = *s0;
	*s0 = *s1;
	*s1 = s_tmp;
	p_tmp = *p0;
	*p0 = *p1;
	*p1 = p_tmp;
}

inline __device__ void _exch_intxn({% for i in range(0,regsort_nr_el_per_thread) %}statetype *s{{i}}, {% endfor %}{% for i in range(0,regsort_nr_el_per_thread) %}shared_indextype *p{{i}}, {% endfor %}uint8_t mask, bool bit) {
	statetype ex_s0, ex_s1;
	shared_indextype ex_p0, ex_p1;
	{% if regsort_nr_el_per_thread > 2 %}
	{% for i in range (0,(regsort_nr_el_per_thread/2)|int) %}
	if (bit) SWP(s{{i}}, s{{(regsort_nr_el_per_thread-1-((i/2)|int*2)-(1-(i%2)))|int}}, p{{i}}, p{{(regsort_nr_el_per_thread-1-((i/2)|int*2)-(1-(i%2)))|int}});
	{% endfor %}
	{% endif %}
	{% for i in range(0,(regsort_nr_el_per_thread/2)|int) %}
	ex_s0 = *s{{i*2}};
	ex_s1 = __shfl_xor_sync(0xFFFFFFFF, {% if regsort_nr_el_per_thread > 1 %}*s{{i*2+1}}{% else %}*s{{i*2}}{% endif %}, mask);
	ex_p0 = *p{{i*2}};
	ex_p1 = __shfl_xor_sync(0xFFFFFFFF, {% if regsort_nr_el_per_thread > 1 %}*p{{i*2+1}}{% else %}*p{{i*2}}{% endif %}, mask);
	CMP_SWP(ex_s0, ex_s1, ex_p0, ex_p1);
	if (bit) EQL_SWP(ex_s0, ex_s1, ex_p0, ex_p1);
	*s{{i*2}} = ex_s0;
	{% if regsort_nr_el_per_thread > 1 %}
	*s{{i*2+1}} = __shfl_xor_sync(0xFFFFFFFF, ex_s1, mask);
	{% endif %}
	*p{{i*2}} = ex_p0;
	{% if regsort_nr_el_per_thread > 1 %}
	*p{{i*2+1}} = __shfl_xor_sync(0xFFFFFFFF, ex_p1, mask);
	{% endif %}
	{% endfor %}
	{% if regsort_nr_el_per_thread > 2 %}
	{% for i in range (0,(regsort_nr_el_per_thread/2)|int) %}
	if (bit) SWP(s{{i}}, s{{(regsort_nr_el_per_thread-1-((i/2)|int*2)-(1-(i%2)))|int}}, p{{i}}, p{{(regsort_nr_el_per_thread-1-((i/2)|int*2)-(1-(i%2)))|int}});
	{% endfor %}
	{% endif %}
}

inline __device__ void _exch_paral({% for i in range(0,regsort_nr_el_per_thread) %}statetype *s{{i}}, {% endfor %}{% for i in range(0,regsort_nr_el_per_thread) %}shared_indextype *p{{i}}, {% endfor %}uint8_t mask, bool bit) {
	statetype ex_s0, ex_s1;
	shared_indextype ex_p0, ex_p1;
	{% if regsort_nr_el_per_thread > 1 %}
	{% for i in range (0,(regsort_nr_el_per_thread/2)|int) %}
	if (bit) SWP(s{{i*2}}, s{{i*2+1}}, p{{i*2}}, p{{i*2+1}});
	{% endfor %}
	{% endif %}
	{% for i in range(0,(regsort_nr_el_per_thread/2)|int) %}
	ex_s0 = *s{{i*2}};
	ex_s1 = __shfl_xor_sync(0xFFFFFFFF, {% if regsort_nr_el_per_thread > 1 %}*s{{i*2+1}}{% else %}*s{{i*2}}{% endif %}, mask);
	ex_p0 = *p{{i*2}};
	ex_p1 = __shfl_xor_sync(0xFFFFFFFF, {% if regsort_nr_el_per_thread > 1 %}*p{{i*2+1}}{% else %}*p{{i*2}}{% endif %}, mask);
	CMP_SWP(ex_s0, ex_s1, ex_p0, ex_p1);
	if (bit) EQL_SWP(ex_s0, ex_s1, ex_p0, ex_p1);
	*s{{i*2}} = ex_s0;
	{% if regsort_nr_el_per_thread > 1 %}
	*s{{i*2+1}} = __shfl_xor_sync(0xFFFFFFFF, ex_s1, mask);
	{% endif %}
	*p{{i*2}} = ex_p0;
	{% if regsort_nr_el_per_thread > 1 %}
	*p{{i*2+1}} = __shfl_xor_sync(0xFFFFFFFF, ex_p1, mask);
	{% endif %}
	{% endfor %}
	{% if regsort_nr_el_per_thread > 1 %}
	{% for i in range (0,(regsort_nr_el_per_thread/2)|int) %}
	if (bit) SWP(s{{i*2}}, s{{i*2+1}}, p{{i*2}}, p{{i*2+1}});
	{% endfor %}
	{% endif %}
}

// The main bitonic sorting function, including loading the data to be sorted,
// and returning the tile index of the element to be subsequently used by the calling thread.
__device__ shared_indextype get_sorted_opentile_element(uint8_t wid) {
	statetype {% for i in range(0,regsort_nr_el_per_thread) %}s{{i}}{% if not loop.last %}, {% else %};{% endif %}{% endfor %}
	
	shared_indextype {% for i in range(0,regsort_nr_el_per_thread) %}p{{i}}, {% endfor %}p_tmp1, p_tmp2, p_result;
	
	// Load the tile indices.
	{% for i in range(0,regsort_nr_el_per_thread) %}
	{% if loop.last %}
	if ({{i*warpsize}}+LANE < OPENTILELEN) {
		asm("{\n\t"
			" cvt.u16.u32 %0, %1;\n\t"
			"}" : "=r"(p{{i}}) : "r"(shared[OPENTILEOFFSET+{{i*warpsize}}+LANE]));
		if (p{{i}} != EMPTYVECT16) {
			// Retrieve corresponding state value.
			get_current_state(&s{{i}}, p{{i}}, wid / {{nr_warps_per_tile}});
		}
		else {
			s{{i}} = NO_STATE;
		}
		p{{i}} = {{i*warpsize}}+LANE;
	}
	else {
		p{{i}} = EMPTYVECT16;
		s{{i}} = NO_STATE;
	}
	{% else %}
	asm("{\n\t"
		" cvt.u16.u32 %0, %1;\n\t"
		"}" : "=r"(p{{i}}) : "r"(shared[OPENTILEOFFSET+{{i*warpsize}}+LANE]));
	if (p{{i}} != EMPTYVECT16) {
		// Retrieve corresponding state value.
		get_current_state(&s{{i}}, p{{i}}, wid / {{nr_warps_per_tile}});
	}
	else {
		s{{i}} = NO_STATE;
	}
	p{{i}} = {{i*warpsize}}+LANE;
	{% endif %}
	{% endfor %}
	// Perform the sorting.
	{% for l in range(1, (regsort_nr_el_per_thread*warpsize)|log2+1)|reverse %}
	{% set coop_elem_num = (l-1)|pow2 %}
	{% set coop_thrd_num = ([warpsize|log2, l-1]|min)|pow2 %}
	{% set coop_elem_size = ((regsort_nr_el_per_thread*warpsize)|log2-l+1)|pow2 %}
	{% set coop_thrd_size = (warpsize|log2 - [warpsize|log2, l-1]|min)|pow2 %}
	{% if coop_thrd_size == 1 %}
	// exch_local intxn.
	{% set rmask = coop_elem_size - 1 %}
	{% set ns = namespace(L=[]) %}
	{% for i in range(0,regsort_nr_el_per_thread) %}
	{% if not i|in_list(ns.L) %}
	CMP_SWP(&s{{i}}, &s{{i|xor(rmask)}}, &p{{i}}, &p{{i|xor(rmask)}});
	{% set ns.L = ns.L + [i, i|xor(rmask)] %}
	{% endif %}
	{% endfor %}
	{% else %}
	_exch_intxn({% for i in range(0,regsort_nr_el_per_thread) %}&s{{i}}, {% endfor %}{% for i in range(0,regsort_nr_el_per_thread) %}&p{{i}}, {% endfor %}{{(coop_thrd_size-1)|hexa}}, (LANE & {{(((coop_thrd_size-1)|log2)|pow2)|hexa}}) != 0);
	{% endif %}
	{% for k in range(l+1, (regsort_nr_el_per_thread*warpsize)|log2+1) %}
	{% set coop_elem_num = (k-1)|pow2 %}
	{% set coop_thrd_num = ([warpsize|log2, k-1]|min)|pow2 %}
	{% set coop_elem_size = ((regsort_nr_el_per_thread*warpsize)|log2-k+1)|pow2 %}
	{% set coop_thrd_size = (warpsize|log2 - [warpsize|log2, k-1]|min)|pow2 %}
	{% if coop_thrd_size == 1 %}
	// exch_local paral.
	{% set rmask = coop_elem_size - 1 %}
	{% set rmask = rmask - rmask|bitshift_one_right %}
	{% set ns = namespace(L=[]) %}
	{% for i in range(0,regsort_nr_el_per_thread) %}
	{% if not i|in_list(ns.L) %}
	CMP_SWP(&s{{i}}, &s{{i|xor(rmask)}}, &p{{i}}, &p{{i|xor(rmask)}});
	{% set ns.L = ns.L + [i, i|xor(rmask)] %}
	{% endif %}
	{% endfor %}
	{% else %}
	{% set tmask = coop_thrd_size - 1 %}
	{% set tmask = tmask - tmask|bitshift_one_right %}
	_exch_paral({% for i in range(0,regsort_nr_el_per_thread) %}&s{{i}}, {% endfor %}{% for i in range(0,regsort_nr_el_per_thread) %}&p{{i}}, {% endfor %}{{tmask|hexa}}, (LANE & {{(((coop_thrd_size - 1)|log2)|pow2)|hexa}}) != 0);
	{% endif %}
	{% endfor %}
	{% endfor %}

	// Finally, retrieve the index of the tile element of interest for the current thread.
	uint8_t offset = wid % {{nr_warps_per_tile}};
	{% for i in range(0,regsort_nr_el_per_thread) %}
	if (LANE*{{regsort_nr_el_per_thread}}+{{i}} >= offset && LANE*{{regsort_nr_el_per_thread}}+{{i}} < offset+WARP_SIZE) {
		p_tmp_1 = p{{i}};
	}
	__syncwarp();
	p_tmp_2 = __shfl_sync(0xFFFFFFFF, p_tmp_1, (offset+LANE) / {{regsort_nr_el_per_thread}});
	if ((offset+LANE) & {{regsort_nr_el_per_thread-1}} == {{i}}) {
		// Value of interest is ready to be fetched.
		p_result = p_tmp_2;
	}
	{% endfor %}
	return p_result;
}

//*** END FUNCTIONS FOR INTRA-WARP BITONIC MERGESORT ***

// Data retrieval functions. Retrieve particular state info from the given state vector part(s).
// Precondition: the given parts indeed contain the requested info.
{% for s in vectorelem_in_structure_map.keys() %}
{% set size = vectorelem_in_structure_map[s][0] %}
inline __device__ void get_{{s|replace("[","_")|replace("]","")|replace("'","_")}}({% if s|is_state %}elem_statetype{% elif size < 32 %}elem_chartype{% else %}elem_inttype{% endif %} *b, inttype part1, inttype part2) {
	asm("{\n\t"
{% if size < 32 %}
		" .reg .u32 t1;\n\t"
{% if vectorelem_in_structure_map[s]|length == 2 %}
		" bfe.u32 t1, %1, {{vectorelem_in_structure_map[s][1][1]}}, {{vectorelem_in_structure_map[s][1][2]}};\n\t"
{% else %}
		" bfe.u32 t1, %2, {{vectorelem_in_structure_map[s][2][1]}}, {{vectorelem_in_structure_map[s][2][2]}};\n\t"
		" bfi.b32 t1, %1, t1, {{vectorelem_in_structure_map[s][2][2]}}, {{vectorelem_in_structure_map[s][1][2]}};\n\t"
{% endif %}
		" cvt.s8.u32 %0, t1;\n\t"
	    "}" : "=r"(*b) : "l"(part1), "l"(part2));
{% else %}
{% if vectorelem_in_structure_map[s]|length == 2 %}
		" bfe.s32 %0, %1, {{vectorelem_in_structure_map[s][1][1]}}, {{vectorelem_in_structure_map[s][1][2]}};\n\t"
	    "}" : "=r"(*b) : "l"(part1), "l"(part2));
{% else %}
		" bfe.s32 %0, %2, {{vectorelem_in_structure_map[s][2][1]}}, {{vectorelem_in_structure_map[s][2][2]}};\n\t"
		" bfi.b32 %0, %1, %0, {{vectorelem_in_structure_map[s][2][2]}}, {{vectorelem_in_structure_map[s][1][2]}};\n\t"
	    "}" : "+r"(*b) : "l"(part1), "l"(part2));
{% endif %}
{% endif %}
}

{% endfor %}
{% if arraynames|length > 0 %}
// Data retrieval functions, including the fetching of required vector parts, for array elements with dynamic indexing.
{% for vname, t, size in arraynames %}
inline __device__ void get_{{vname|replace("[","_")|replace("]","")|replace("'","_")}}(shared_indextype node_index, {% if t|gettypesize < 32 %}elem_chartype{% else %}elem_inttype{% endif %} *b, array_indextype index) {
	inttype part1, part2;
	switch (index) {
		{% for i in range(0, size) %}
		case {{i}}:
			// Retrieve correct vector part(s).
			{% set PIDs = vectorelem_in_structure_map[vname + "[" + i|string + "]"] %}
			part1 = get_vectorpart_{{PIDs[1][0]}}(node_index);
			{% if PIDs|length > 2 %}
			part2 = get_vectorpart_{{PIDs[2][0]}}(node_index);
			{% endif %}
			// Get the data.
			get_{{vname|replace("[","_")|replace("]","") + "_" + i|string}}(b, part1, part2);
			break;
		{% endfor %}
		default:
			break;
	}
}

{% endfor %}
{% endif %}
// Retrieval of current state of state machine at position i in state vector.
inline __device__ void get_current_state(statetype *b, shared_indextype node_index, elem_chartype i) {
	inttype part1, part2;
	switch (i) {
		{% for i in range(0,smnames|length) %}
		case {{i}}:
			{% set vinfo = vectorelem_in_structure_map[state_order[i]] %}
			part1 = get_vectorpart_{{vinfo[1][0]}}(node_index);
			{% if vinfo|length > 2 %}
			part2 = get_vectorpart_{{vinfo[2][0]}}(node_index);
			{% endif %}
			get_{{state_order[i]|replace("'","_")}}(b, part1, part2);
		{% endfor %}
		default:
			break;
	}
}

// Data update functions. Update particular state info in the given state vector part(s).
// Precondition: the given part indeed needs to contain the indicated fragment (left or right in case the info is split over two parts) of the updated info.
{% for s in vectorelem_in_structure_map.keys() %}
{% set size = vectorelem_in_structure_map[s][0] %}
inline __device__ void set_left_{{s|replace("[","_")|replace("]","")|replace("'","_")}}(inttype *part, {% if size < 32 %}elem_chartype{% else %}elem_inttype{% endif %} x) {
	asm("{\n\t"
		" .reg .u64 t1;\n\t"
{% if size < 32 %}
		" cvt.u64.u8 t1, %1\n\t"
{% else %}
		" cvt.u64.u32 t1, %1\n\t"
{% endif %}
{% if vectorelem_in_structure_map[s]|length > 2 %}
		" shr.b64 t1, t1, {{vectorelem_in_structure_map[s][2][2]}};\n\t"
{% endif %}
		" bfi.b64 %0, t1, %0, {{vectorelem_in_structure_map[s][1][1]}}, {{vectorelem_in_structure_map[s][1][2]}};\n\t"
		"}" : "+l"(*part) : "r"(x);
}

{% if vectorelem_in_structure_map[s]|length > 2 %}
inline __device__ void set_right_{{s|replace("[","_")|replace("]","")|replace("'","_")}}(inttype *part, {% if size < 32 %}elem_chartype{% else %}elem_inttype{% endif %} x) {
	asm("{\n\t"
		" .reg .u64 t1;\n\t"
{% if size < 32 %}
		" cvt.u64.u8 t1, %1\n\t"
{% else %}
		" cvt.u64.u32 t1, %1\n\t"
{% endif %}
		" bfi.b64 %0, t1, %0, {{vectorelem_in_structure_map[s][2][1]}}, {{vectorelem_in_structure_map[s][2][2]}};\n\t"
		"}" : "+l"(*part) : "r"(x);
}

{% endif %}
{% endfor %}
// Data update functions for arrays with dynamic indexing, focussed on one specific vector part.
{% for x in dynamic_write_arrays.keys() %}
// Auxiliary functions for {{dynamic_write_arrays[x][0]}}.
inline __device__ bool array_element_is_in_vectorpart_{{dynamic_write_arrays[x][0]|replace("'","_")}}(array_indextype i, vectornode_indextype pid) {
	switch (pid) {
		{% for p in range(dynamic_write_arrays[x][1], dynamic_write_arrays[x][2]+1) %}
		case {{p}}:
			return (i >= {{(x|get_array_range_in_vectorpart(dynamic_write_arrays[x][0],p))[0]}} && i <= {{(x|get_array_range_in_vectorpart(dynamic_write_arrays[x][0],p))[1]}});
		{% endfor %}
		default:
			return false;
	}
}

// Precondition: array element i is (partially) stored in vector part pid.
inline __device__ bool is_left_vectorpart_for_array_element_{{dynamic_write_arrays[x][0]|replace("'","_")}}(array_indextype i, vectornode_indextype pid) {
	switch (pid) {
		{% for p in range(dynamic_write_arrays[x][1], dynamic_write_arrays[x][2]+1) %}
		case {{p}}:
			return (i >{% if loop.first %}= 0{% else %} {{(x|get_array_range_in_vectorpart(dynamic_write_arrays[x][0],p-1))[1]}}{% endif %} && i <= {{(x|get_array_range_in_vectorpart(dynamic_write_arrays[x][0],p))[1]}});
		{% endfor %}
		default:
			return false;
	}	
}

// Data update function for {{dynamic_write_arrays[x][0]}}.
inline __device__ inttype set_{{dynamic_write_arrays[x][0]|replace("'","_")}}(inttype *part, array_indextype indices, {{x.type|cudatype(True)}} buf, buffer_indextype buf_offset, vectornode_indextype part_id) {
	if (part_id >= {{dynamic_write_arrays[x][1]}} && part_id <= {{dynamic_write_arrays[x][2]}}) {
		for (array_indextype i = 0;; i++) {
			if (indices[i] == EMPTY_INDEX) {
				break;
			}
			if (array_element_is_in_vectorpart_{{dynamic_write_arrays[x][0]|replace("'","_")}}(indices[i], part_id)) {
				if (is_left_vectorpart_for_array_element_{{dynamic_write_arrays[x][0]|replace("'","_")}}(indices[i], part_id)) {
					switch (indices[i]) {
						{% for i in range(dynamic_write_arrays[x][1], dynamic_write_arrays[x][2]+1) %}
						case {{i}}:
							set_left_{{dynamic_write_arrays[x][0]|replace("'","_")}}_{{i}}(part, buf[buf_offset + i]);
							break;
						{% endfor %}
						default:
							break;
					}
				}
				else {
					switch (indices[i]) {
						{% for i in range(dynamic_write_arrays[x][1], dynamic_write_arrays[x][2]+1) %}
						{% if vectorelem_in_structure_map[dynamic_write_arrays[x][0] + "[" + i|string + "]"]|length > 2 %}
						case {{i}}:
							set_right_{{dynamic_write_arrays[x][0]|replace("'","_")}}_{{i}}(part, buf[buf_offset + i]);
							break;
						{% endif %}
						{% endfor %}
						default:
							break;
					}
				}
			}
		}
	}
}
{% endfor %}

// Data update functions for channel buffers, focussed on one specific vector part.
{% for c in model.channels|select("is_async") %}
// Data update function for parameter i of tail element of the buffer of {{c.name}}.
{% for i in range(0,c.type|length+1) %}
{% if i > 0 or signalsize[c] > 0 %}
inline __device__ void set_buffer_tail_element_{{c.name}}_{{i}}(inttype *part, buffer_indextype size, {% if i == 0 %}{% if signalsize[c] <= 8 %}uint8_t{% elif signalsize[c] <= 16 %}uint16_t{% else %}uint32_t{% endif %}{% else %}{{(c.type[i-1])|cudatype(True)}}{% endif %} value, vectornode_indextype part_id) {
	switch (size) {
		{% for n in range(0,c.size) %}
		case {{n}}:
			switch (part_id) {
				{% set PIDs = vectorelem_in_structure_map.get(c.name + "[" + i|string + "][" + n|string + "]") %}
				{% if PIDs != None %}
				case {{PIDs[1][0]}}:
					set_left_{{c.name}}_{{i}}_{{n}}(part, value);
					break;
				{% if PIDs|length > 2 %}
				case {{PIDs[2][0]}}:
					set_right_{{c.name}}_{{i}}_{{n}}(part, value);
					break;
				{% endif %}
				{% endif %}
				default:
					break;
			}
		{% endfor %}
		default:
			break;
	}
}

{% endif %}
{% endfor %}
// Data update function shifting each element of the buffer of {{c.name}} one position towards the head, insofar this is relevant for the given vectorpart.
inline __device__ void shift_buffer_tail_elements_{{c.name}}(shared_indextype node_index, shared_inttype *part, buffer_indextype size, vectornode_indextype part_id) {
	{% if c.size > 1 %}
	inttype part_tmp;
	{% if signalsize[c] > 0 %}
	{% if signalsize <= 2 %}
	bool signal_tmp;
	{% elif signalsize <= 8 %}
	uint8_t signal_tmp;
	{% elif signalsize <= 16 %}
	uint16_t signal_tmp;
	{% elif signalsize <= 32 %}
	uint32_t signal_tmp;
	{% endif %}
	{% endif %}
	{% for t in c.type %}
	{% if t.base == 'Integer' %}
	elem_inttype int_tmp;{{break}}
	{% endif %}
	{% endfor %}
	{% for t in c.type %}
	{% if t.base == 'Byte' %}
	elem_chartype char_tmp;{{break}}
	{% endif %}
	{% endfor %}
	{% for t in c.type %}
	{% if t.base == 'Boolean' %}
	elem_booltype bool_tmp;{{break}}
	{% endif %}
	{% endfor %}

	switch (part_id) {
		{% set parts = async_channel_vectorpart_buffer_range[c] %}
		{% for p in parts.keys() %}
		case {{p}}:
			{% if parts.get(p+1) != None %}
			part_tmp = get_vectorpart(node_index, part_id+1);
			{% endif %}
			{% set lower1 = parts[p][0][0] %}
			{% set lower2 = parts[p][0][1] %}
			{% set upper1 = parts[p][1][0] %}
			{% set upper2 = parts[p][1][1] %}
			{% for i in range(0,c.size) %}
			{% for j in range(0,c.type|length+1) %}
			{% if (lower2 < i or (lower2 == i and lower1 <= j)) and (upper2 > i or (upper2 == i and upper1 >= j)) and c|next_buffer_element(j,i) != (-1,-1) and (j > 0 or signalsize[c] > 0) %}
			if ({{i+1}} < size) {
			{% set (nj,ni) = c|next_buffer_element(j,i) %}
			{% if upper2 > ni or (upper2 == ni and upper1 >= nj) %}
				get_{{c.name}}_{{nj}}_{{ni}}(&{% if nj == 0 %}signal_tmp{% else %}{% if c.type[nj-1].base == 'Integer' %}int_tmp{% elif c.type[nj-1].base == 'Byte' %}char_tmp{% else %}bool_tmp{% endif %}{% endif %}, *part, part_tmp);
			{% else %}
				get_{{c.name}}_{{nj}}_{{ni}}(&{% if nj == 0 %}signal_tmp{% else %}{% if c.type[nj-1].base == 'Integer' %}int_tmp{% elif c.type[nj-1].base == 'Byte' %}char_tmp{% else %}bool_tmp{% endif %}{% endif %}, part_tmp, *part);
			{% endif %}
			{% set PIDs = vectorelem_in_structure_map[c.name + "[" + j|string + "][" + i|string + "]"] %}
			{% if PIDs[1][0] == p %}
				set_left_{{c.name}}_{{j}}_{{i}}(part, {% if nj == 0 %}signal_tmp{% else %}{% if c.type[nj-1].base == 'Integer' %}int_tmp{% elif c.type[nj-1].base == 'Byte' %}char_tmp{% else %}bool_tmp{% endif %}{% endif %});
			{% else %}
				set_right_{{c.name}}_{{j}}_{{i}}(part, {% if nj == 0 %}signal_tmp{% else %}{% if c.type[nj-1].base == 'Integer' %}int_tmp{% elif c.type[nj-1].base == 'Byte' %}char_tmp{% else %}bool_tmp{% endif %}{% endif %});
			{% endif %}
			}
			else {
				set_left_{{c.name}}_{{j}}_{{i}}(part, 0);
				break;
			}
			{% endif %}
			{% endfor %}
			{% endfor %}
			break;
		{% endfor %}
		default:
			break;
	}
	{% else %}
	return;
	{% endif %}
}

{% endfor %}
// Auxiliary functions to check for and obtain/store an array element with an index equal to the given expression e.
// There are functions for the various buffer sizes required to interpret the model.

// Store the given value v under index e. Check for presence of e in the index buffer. If not present, store e and v.
// Precondition: if e is not already present, there is space in the buffer to store it.
{% for n in all_arrayindex_allocs_sizes %}
template<class T>
inline __device__ void A_STR_{{n}}({% for i in range(0,n) %}array_indextype *idx_{{i}}, {% endfor %}{% for i in range(0,n) %}T *v_{{i}}, {% endfor %}array_indextype e, T v) {
{% for i in range(0,n) %}
{% if not loop.first%}	else {% else %}	{% endif %}if (((array_indextype) e) == *idx_{{i}}) {
		*v_{{i}} = v;
		return;
	}
	else if (*idx_{{i}} == EMPTY_INDEX) {
		*idx_{{i}} = (array_indextype) e;
		*v_{{i}} = v;
		return;
	}
{% endfor %}
}

{% endfor %}
// Return the value stored at index e.
// Precondition: provided array contains the requested element.
{% for n in all_arrayindex_allocs_sizes %}
template<class T>
inline __device__ T A_LD_{{n}}({% for i in range(0,n) %}array_indextype idx_{{i}}, {% endfor %}{% for i in range(0,n) %}T v_{{i}}, {% endfor %}array_indextype e) {
{% for i in range(0,n) %}
{% if not loop.first%}	else {% else %}	{% endif %}if (((array_indextype) e) == idx_{{i}}) {
		return v_{{i}};
	}
{% endfor %}
	return T();
}

{% endfor %}

// Check whether the given array index e is stored in the given array index buffer.
{% for n in all_arrayindex_allocs_sizes %}
inline __device__ bool A_IEX_{{n}}({% for i in range(0,n) %}array_indextype idx_{{i}}, {% endfor %}array_indextype e) {
{% for i in range(0,n) %}
{% if not loop.first%}	else {% else %}	{% endif %}if (((array_indextype) e) == idx_{{i}}) {
		return true;
	}
{% endfor %}
	return false;
}

{% endfor %}
{% for c in model.classes %}
{% set cloop = loop %}
{% for sm in c.statemachines %}
{% set smloop = loop %}
{% for a in alphabet[sm] if a in syncactions %}
{% if cloop.first and smloop.first and loop.first %}
// Action execution functions. For each state machine and action requiring synchronisation, there is a function returning for a given
// source state a target state that can be reached by performing the action. In case of non-determinism, repeated calls of the function
// will produce each of the different reachable states.
{% endif %}
inline __device__ statetype get_target_{{sm.name}}_{{a}}(statetype src, statetype prev_tgt) {
	switch (src) {
		{% set atrans = actiontargets[sm][a] %}
		{% for src, tgts in atrans|dictsort %}
		case {{src}}:
			switch (prev_tgt) {
				{% for j in range(0, tgts|length+1) %}
				{% if j == 0 %}
				case -1:
				{% else %}
				case {{tgts[j-1]}}:
				{% endif %}
					{% if j == tgts|length %}
					return -1;
					{% else %}
					return {{tgts[j]}};
					{% endif %}
				{% endfor %}
				default:
					return -1;
			}
		{% endfor %}
		default:
			return -1;
	}
}

{% endfor %}
{% endfor %}
{% endfor %}
// Kernel function to start parallel successor generation. Precondition: a tile of vectortree pointers to roots of cache-preloaded vectortrees
// is stored in the shared memory.
inline __device__ void GENERATE_SUCCESSORS() {
	// Iterate over the designated work.
	{% if gpuexplore2_succdist %}
	shared_inttype src_state = shared[OPENTILEOFFSET+GROUP_ID];

	if (THREADINGROUP) {
		if (ISVECTORSTATE(src_state)) {
			get_successors_of_sm((shared_indextype) src_state, GROUP_TID);
		}
	}
	{% elif no_regsort %}
	shared_inttype src_state = shared[OPENTILEOFFSET+OPENTILE_ELEM];
	if (ISVECTORSTATE(src_state)) {
		for (shared_indextype i = WARP_ID; (i/{{nr_warps_per_tile}}) < {{smnames|length}}; i += NR_WARPS_PER_BLOCK) {
			get_successors_of_sm((shared_indextype) src_state, (i/{{nr_warps_per_tile}}));
		}
	}
	{% else %}
	for (elem_chartype i = WARP_ID; (i/{{nr_warps_per_tile}}) < {{smnames|length}}; i += NR_WARPS_PER_BLOCK) {
		shared_inttype src_state = get_sorted_opentile_element(i);
		if (ISVECTORSTATE(src_state)) {
			get_successors_of_sm((shared_indextype) src_state, (i/{{nr_warps_per_tile}}));
		}
	}
	{% endif %}
} 

// Successor construction function for a particular state machine. Given a state vector, construct its successor state vectors w.r.t. the state machine, and store them in cache.
// Vgtid is the identity of the thread calling the function (id of thread relevant for successor generation).
inline __device__ void get_successors_of_sm(shared_indextype node_index, uint8_t vgtid) {
	// explore the outgoing transitions of the current state of the state machine assigned to vgtid.
	switch (vgtid) {
		{% for i in range(0,smnames|length) %}
		case {{i}}:
			explore_{{state_order[i]|replace("'","_")}}(node_index);
			break;
		{% endfor %}
		default:
			break;
	}
}

// Exploration functions to traverse outgoing transitions of the various states.
{% for i in range(0,smnames|length) %}
inline __device__ void explore_{{state_order[i]|replace("'","_")}}(shared_indextype node_index) {
	// Fetch the current state of the state machine.
	get_current_state(&current, node_index, {{i}});
	statetype target = NO_STATE;
	inttype part1, part2;
	shared_inttype part_d_cachepointers;
	switch (current) {
		{% for s in smname_to_object[state_order[i]][1].states %}
		{% set o = smname_to_object[state_order[i]][0] %}
		{% set sm = smname_to_object[state_order[i]][1] %}
		{% if s|nr_of_transitions_to_be_processed_by(i,o) > 0 %}
		case {{state_id[s]}}:
			{% set indent = "" %}
			{% set buffer_allocs = s|object_trans_to_be_processed_by_sm_thread(o)|get_buffer_allocs %}
			{% if buffer_allocs[0] + buffer_allocs[1] + buffer_allocs[2] + buffer_allocs[3] + buffer_allocs[4] > 0 %}
			// Allocate register memory to process transition(s).
			{% endif %}
			{% if buffer_allocs[0] > 0 %}
			elem_inttype {% for j in range(0,buffer_allocs[0]) %}buf32_{{j}}{{', ' if not loop.last}}{% endfor %};
			{% endif %}
			{% if buffer_allocs[1] > 0 %}
			shared_indextype {% for j in range(0,buffer_allocs[1]) %}buf16_{{j}}{{', ' if not loop.last}}{% endfor %};
			{% endif %}
			{% if buffer_allocs[2] > 0 %}
			elem_chartype {% for j in range(0,buffer_allocs[2]) %}buf8_{{j}}{{', ' if not loop.last}}{% endfor %};
			{% endif %}
			{% if buffer_allocs[3] > 0 %}
			bool {% for j in range(0,buffer_allocs[3]) %}buf1_{{j}}{{', ' if not loop.last}}{% endfor %};
			{% endif %}
			{% if buffer_allocs[4] > 0 %}
			// Allocate register memory for dynamic array indexing.
			array_indextype {% for j in range(0,buffer_allocs[4]) %}idx_{{j}}{{', ' if not loop.last}}{% endfor %};
			{% endif %}
			{% set ns = namespace(lastprio=100, prio_nestings=0) %}
			{% for t in s|outgoingtrans(sm.transitions) %}
			{% if t|must_be_processed_by(i,o) %}
			{% if t.priority > ns.lastprio %}
			if (target == NO_STATE) {
			{% set ns.prio_nestings = ns.prio_nestings + 1 %}
			{% set indent = indent + "\t" %}
			{% endif %}
			{% set ns.lastprio = t.priority %}
			{% set st = t.statements[0] %}
			{% set allocs = t|get_buffer_arrayindex_allocs(o) %}
			
			{{indent}}// {{s.name}} --{ {{st|getlabel}} }--> {{t.target.name}}
			
			{% set M = t|map_variables_on_buffer(o,buffer_allocs) %}
			{% if st.__class__.__name__ == 'ReceiveSignal' %}
			{% if st|cudarecsizeguard(M,o) != "" %}
			{{indent}}// Fetch buffer size value.
			{% set ch = connected_channel[(o, st.target)] %}
			{% set VP = [(ch,"_size")]|get_vectorparts(o) %}
			{{indent}}part1 = get_vectorpart(node_index, {{VP[0]}});
			{% if VP|length > 1 %}
			{{indent}}part2 = get_vectorpart(node_index, {{VP[1]}});
			{% endif %}
			{{indent}}get_{{ch.name}}_size(&{{M[(ch,"_size")][0]}}_{{M[(ch,"_size")][1]}}, part1, part2);
			{{indent}}if ({{st|cudarecsizeguard(M,o)}}) {
			{% set indent = indent + "\t" %}
			{% endif %}
			{% endif %}
			{% set fetchcode = st|cudafetchdata(indent|length+3, o, M, True, True) %}
			{% if fetchcode != "" %}
			{{indent}}{{fetchcode}}
			{% endif %}
			{{indent}}// Statement computation.
			{% if st|cudaguard(M,o) != "" %}
			{{indent}}if ({{st|cudaguard(M,o)}}) {
			{% set indent = indent + "\t" %}
			{% endif %}
			{% set fetchcode = st|cudafetchdata(indent|length+3, o, M, False, False) %}
			{% if fetchcode != "" %}
			{{indent}}{{fetchcode}}
			{% endif %}
			{% if st|cudastatement(1,o,M) != "" %}
			{{indent}}{{st|cudastatement(indent|length+3,o,M)}}{% endif %}
			{% if st|cudaguard(M,o) != "" %}
			{% set indent = indent[:-1] %}
			{{indent}}}
			{% endif %}
			{% if st.__class__.__name__ == "ReceiveSignal" %}
			{% if st|cudarecsizeguard(M,o) != "" %}
			}
			{% endif %}
			{% endif %}
			{% endif %}
			{% for j in range(0, ns.prio_nestings) %}
			{% set indent = indent[:-1] %}
			{{indent}}}
			{% endfor %}
			{% endfor %}
			break;
		{% endif %}
		{% endfor %}
		default:
			break;
	}
}

{% endfor %}